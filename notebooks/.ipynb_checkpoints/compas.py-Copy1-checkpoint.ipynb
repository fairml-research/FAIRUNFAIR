{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc44862c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T23:40:52.697581Z",
     "start_time": "2024-04-26T23:40:52.681600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3bd7926-fa63-4099-bd23-386357e2873a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!conda create --name fair_env python=3.7 -y\n",
    "#!conda activate fair_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "939a5fc2-7fc7-4bf4-9d53-f7b102606dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cycler==0.10.0 (from -r requirements.txt (line 1))\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl.metadata (722 bytes)\n",
      "Collecting decorator==4.0.11 (from -r requirements.txt (line 2))\n",
      "  Using cached decorator-4.0.11-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting matplotlib==2.0.0 (from -r requirements.txt (line 3))\n",
      "  Using cached matplotlib-2.0.0.tar.gz (53.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting networkx==1.11 (from -r requirements.txt (line 4))\n",
      "  Using cached networkx-1.11-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting numpy==1.14.0 (from -r requirements.txt (line 5))\n",
      "  Using cached numpy-1.14.0.zip (4.9 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pandas==0.21.1 (from -r requirements.txt (line 6))\n",
      "  Using cached pandas-0.21.1.tar.gz (11.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25l|^C\n",
      "\u001b[?25canceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3983c298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.16\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb17c74-6d50-4cb9-b5f4-a8b123565857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fairness in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (0.1.8)\n",
      "Requirement already satisfied: BlackBoxAuditing>=0.1.26ggplot in /home/vince/.local/lib/python3.8/site-packages (from fairness) (0.1.54)\n",
      "Requirement already satisfied: fire in /home/vince/.local/lib/python3.8/site-packages (from fairness) (0.5.0)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from fairness) (1.1.5)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from fairness) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.0 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from fairness) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from fairness) (2024.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.1 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from fairness) (1.3.2)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from fairness) (1.10.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from fairness) (1.16.0)\n",
      "Requirement already satisfied: wheel>=0.29.0 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from fairness) (0.41.2)\n",
      "Requirement already satisfied: networkx in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from BlackBoxAuditing>=0.1.26ggplot->fairness) (3.1)\n",
      "Requirement already satisfied: matplotlib in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from BlackBoxAuditing>=0.1.26ggplot->fairness) (3.7.5)\n",
      "Requirement already satisfied: numpy in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from BlackBoxAuditing>=0.1.26ggplot->fairness) (1.18.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from scikit-learn>=0.18.1->fairness) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from scikit-learn>=0.18.1->fairness) (3.4.0)\n",
      "Collecting numpy (from BlackBoxAuditing>=0.1.26ggplot->fairness)\n",
      "  Using cached numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: termcolor in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from fire->fairness) (2.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (10.2.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (6.3.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/vince/anaconda3/envs/llamaind/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (3.18.1)\n",
      "Using cached numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[33mDEPRECATION: fairness 0.1.8 has a non-standard dependency specifier BlackBoxAuditing>=0.1.26ggplot. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of fairness or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.0\n",
      "    Uninstalling numpy-1.18.0:\n",
      "      Successfully uninstalled numpy-1.18.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "seaborn 0.13.2 requires pandas>=1.2, but you have pandas 1.1.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.4\n"
     ]
    }
   ],
   "source": [
    "!pip3 install fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c42f4c08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fairness in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (0.1.8)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (1.7.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (2.9.0.post0)\n",
      "Requirement already satisfied: BlackBoxAuditing>=0.1.26ggplot in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (0.1.54)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (3.1.2)\n",
      "Requirement already satisfied: fire in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (0.6.0)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (1.3.5)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (1.16.0)\n",
      "Requirement already satisfied: wheel>=0.29.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (0.38.4)\n",
      "Requirement already satisfied: pytz in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (2024.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.1 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (1.0.2)\n",
      "Requirement already satisfied: networkx in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from BlackBoxAuditing>=0.1.26ggplot->fairness) (2.6.3)\n",
      "Requirement already satisfied: numpy in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from BlackBoxAuditing>=0.1.26ggplot->fairness) (1.21.6)\n",
      "Requirement already satisfied: matplotlib in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from BlackBoxAuditing>=0.1.26ggplot->fairness) (3.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from scikit-learn>=0.18.1->fairness) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from scikit-learn>=0.18.1->fairness) (1.3.2)\n",
      "Requirement already satisfied: termcolor in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fire->fairness) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (24.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (1.4.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (9.5.0)\n",
      "Requirement already satisfied: typing-extensions in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (4.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419492d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f7ad9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fairness in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (0.1.8)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (3.1.2)\n",
      "Requirement already satisfied: BlackBoxAuditing>=0.1.26ggplot in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (0.1.54)\n",
      "Requirement already satisfied: pytz in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (2024.1)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (1.7.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (2.9.0.post0)\n",
      "Requirement already satisfied: fire in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (0.6.0)\n",
      "Requirement already satisfied: wheel>=0.29.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (0.38.4)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (1.3.5)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18.1 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fairness) (1.0.2)\n",
      "Requirement already satisfied: matplotlib in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from BlackBoxAuditing>=0.1.26ggplot->fairness) (3.5.3)\n",
      "Requirement already satisfied: networkx in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from BlackBoxAuditing>=0.1.26ggplot->fairness) (2.6.3)\n",
      "Requirement already satisfied: numpy in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from BlackBoxAuditing>=0.1.26ggplot->fairness) (1.21.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from scikit-learn>=0.18.1->fairness) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from scikit-learn>=0.18.1->fairness) (1.3.2)\n",
      "Requirement already satisfied: termcolor in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from fire->fairness) (2.3.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (9.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (24.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (1.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (4.38.0)\n",
      "Requirement already satisfied: typing-extensions in /home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->BlackBoxAuditing>=0.1.26ggplot->fairness) (4.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cb85c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03831630",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T23:40:58.097464Z",
     "start_time": "2024-04-26T23:40:53.704779Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns: ['two_year_recid', 'sex-race', 'race']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "import fairness\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets as ds\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "#get_ipython().run_line_magic('autoreload', '2')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "from models.zhang import ZHANG\n",
    "from models.adel import ADEL\n",
    "\n",
    "#from models.Broad import BROAD\n",
    "\n",
    "from data.load_fairness_data import LOAD_DATA_TRAIN_TEST\n",
    "#\n",
    "\n",
    "X_train0, X_test0,X_train, X_test, y_train, y_test, S_train, S_test, column_names = LOAD_DATA_TRAIN_TEST('compass')\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class NN_y_adel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN_y_adel, self).__init__()\n",
    "        self.fc1 = nn.Linear(taille_z, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class NN_z_adel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN_z_adel, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, taille_z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class NN_s_adel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN_s_adel, self).__init__()\n",
    "        self.fc1_s = nn.Linear(taille_z, 64)\n",
    "        self.fc2_s = nn.Linear(64, 32)\n",
    "        self.fc3_s = nn.Linear(32, 16)\n",
    "        self.fc4_s = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1_s(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2_s(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3_s(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc4_s(x)\n",
    "        return x\n",
    "\n",
    "taille_z = 64\n",
    "\n",
    "\n",
    "class NN_y(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN_y, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "            \n",
    "\n",
    "class NN_s(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN_s, self).__init__()\n",
    "        self.fc1_s = nn.Linear(1, 64)\n",
    "        self.fc2_s = nn.Linear(64, 32)\n",
    "        self.fc3_s = nn.Linear(32, 16)\n",
    "        self.fc4_s = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1_s(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2_s(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3_s(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc4_s(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "taille_z = 64\n",
    "\n",
    "\n",
    "\n",
    "class NN_y_dro(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN_y_dro, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "            \n",
    "\n",
    "class NN_s_dro(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN_s_dro, self).__init__()\n",
    "        self.fc1_s = nn.Linear(1, 64)\n",
    "        self.fc2_s = nn.Linear(64, 32)\n",
    "        self.fc3_s = nn.Linear(32, 16)\n",
    "        self.fc4_s = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1_s(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2_s(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3_s(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc4_s(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NN_a_dro(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN_a_dro, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1]+1, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = torch.exp(self.fc3(x))\n",
    "        return x \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# # loop results\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "#LAMBDAS = \n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "### MULTIPLE RUNS\n",
    "from utils.fairness_utils import get_subroups_results, get_scores, get_subroups, get_subgroups_random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "scores = []\n",
    "scores_rdm = []\n",
    "\n",
    "N_RUNS = 2\n",
    "\n",
    "N_MIN_GROUP = 50\n",
    "N_EPOCHS = 200\n",
    "#N_MIN_GROUPS = [10, 20, 30, 50, 80, 100, 200]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "addba0e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:40:33.288781Z",
     "start_time": "2024-04-26T21:40:17.838250Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.38841176 lossS 0.26999393 lossY 0.38841176\n",
      "epoch 5 loss 0.7473491 lossS 0.19645604 lossY 0.7473491\n",
      "epoch 10 loss 0.41847175 lossS 0.19717263 lossY 0.41847175\n",
      "epoch 15 loss 0.3644998 lossS 0.16307709 lossY 0.3644998\n",
      "epoch 20 loss 0.13893175 lossS 0.22726539 lossY 0.13893175\n",
      "epoch 25 loss 0.34867305 lossS 0.30899167 lossY 0.34867305\n",
      "epoch 30 loss 0.4009593 lossS 0.17018048 lossY 0.4009593\n",
      "epoch 35 loss 0.30592793 lossS 0.24779636 lossY 0.30592793\n",
      "epoch 40 loss 0.15203562 lossS 0.20086455 lossY 0.15203562\n",
      "epoch 45 loss 0.1619432 lossS 0.29236218 lossY 0.1619432\n",
      "epoch 50 loss 0.24357332 lossS 0.22219238 lossY 0.24357332\n",
      "epoch 55 loss 0.12792917 lossS 0.24367343 lossY 0.12792917\n",
      "epoch 60 loss 0.20569605 lossS 0.20208573 lossY 0.20569605\n",
      "epoch 65 loss 0.23588885 lossS 0.24381676 lossY 0.23588885\n",
      "epoch 70 loss 0.20253721 lossS 0.22169861 lossY 0.20253721\n",
      "epoch 75 loss 0.15461878 lossS 0.30744013 lossY 0.15461878\n",
      "epoch 80 loss 0.23028168 lossS 0.2217682 lossY 0.23028168\n",
      "epoch 85 loss 0.19027682 lossS 0.28816268 lossY 0.19027682\n",
      "epoch 90 loss 0.18982962 lossS 0.28511265 lossY 0.18982962\n",
      "epoch 95 loss 0.16790214 lossS 0.32660288 lossY 0.16790214\n",
      "epoch 100 loss 0.1971122 lossS 0.26228464 lossY 0.1971122\n",
      "epoch 105 loss 0.26137158 lossS 0.16425903 lossY 0.26137158\n",
      "epoch 110 loss 0.2284439 lossS 0.18186425 lossY 0.2284439\n",
      "epoch 115 loss 0.17239508 lossS 0.15620433 lossY 0.17239508\n",
      "epoch 120 loss 0.21285175 lossS 0.175403 lossY 0.21285175\n",
      "epoch 125 loss 0.15662865 lossS 0.17549862 lossY 0.15662865\n",
      "epoch 130 loss 0.15828784 lossS 0.31134894 lossY 0.15828784\n",
      "epoch 135 loss 0.2127853 lossS 0.27203608 lossY 0.2127853\n",
      "epoch 140 loss 0.23153031 lossS 0.3074264 lossY 0.23153031\n",
      "epoch 145 loss 0.16489926 lossS 0.19974588 lossY 0.16489926\n",
      "epoch 150 loss 0.25564158 lossS 0.26565328 lossY 0.25564158\n",
      "epoch 155 loss 0.14858955 lossS 0.21879932 lossY 0.14858955\n",
      "epoch 160 loss 0.17508818 lossS 0.2177341 lossY 0.17508818\n",
      "epoch 165 loss 0.17068914 lossS 0.19820203 lossY 0.17068914\n",
      "epoch 170 loss 0.20708421 lossS 0.23714359 lossY 0.20708421\n",
      "epoch 175 loss 0.16175649 lossS 0.21458036 lossY 0.16175649\n",
      "epoch 180 loss 0.20062006 lossS 0.20836513 lossY 0.20062006\n",
      "epoch 185 loss 0.23529293 lossS 0.21152791 lossY 0.23529293\n",
      "epoch 190 loss 0.20843863 lossS 0.2206956 lossY 0.20843863\n",
      "epoch 195 loss 0.20858167 lossS 0.19838828 lossY 0.20858167\n",
      "epoch 200 loss 0.12230825 lossS 0.17861426 lossY 0.12230825\n",
      "epoch 205 loss 0.21755268 lossS 0.21598314 lossY 0.21755268\n",
      "epoch 210 loss 0.22767206 lossS 0.28303686 lossY 0.22767206\n",
      "epoch 215 loss 0.2162975 lossS 0.23423778 lossY 0.2162975\n",
      "epoch 220 loss 0.15008233 lossS 0.23580615 lossY 0.15008233\n",
      "epoch 225 loss 0.19416688 lossS 0.20668451 lossY 0.19416688\n",
      "epoch 230 loss 0.18039235 lossS 0.21412642 lossY 0.18039235\n",
      "epoch 235 loss 0.1682664 lossS 0.16751361 lossY 0.1682664\n",
      "epoch 240 loss 0.23085685 lossS 0.29274598 lossY 0.23085685\n",
      "epoch 245 loss 0.2097638 lossS 0.2292919 lossY 0.2097638\n",
      "epoch 250 loss 0.22233398 lossS 0.31318572 lossY 0.22233398\n",
      "epoch 255 loss 0.20353691 lossS 0.21407321 lossY 0.20353691\n",
      "epoch 260 loss 0.19961932 lossS 0.20829423 lossY 0.19961932\n",
      "epoch 265 loss 0.15458041 lossS 0.24165253 lossY 0.15458041\n",
      "epoch 270 loss 0.16259547 lossS 0.13967918 lossY 0.16259547\n",
      "epoch 275 loss 0.15341654 lossS 0.24858648 lossY 0.15341654\n",
      "epoch 280 loss 0.31854492 lossS 0.27635905 lossY 0.31854492\n",
      "epoch 285 loss 0.12870473 lossS 0.2189994 lossY 0.12870473\n",
      "epoch 290 loss 0.14098766 lossS 0.17222622 lossY 0.14098766\n",
      "epoch 295 loss 0.20884944 lossS 0.18470144 lossY 0.20884944\n",
      "epoch 300 loss 0.1819158 lossS 0.22058158 lossY 0.1819158\n",
      "epoch 305 loss 0.19822179 lossS 0.23967312 lossY 0.19822179\n",
      "epoch 310 loss 0.14370935 lossS 0.20497434 lossY 0.14370935\n",
      "epoch 315 loss 0.16768174 lossS 0.19828898 lossY 0.16768174\n",
      "epoch 320 loss 0.20788935 lossS 0.18270792 lossY 0.20788935\n",
      "epoch 325 loss 0.11224919 lossS 0.21697979 lossY 0.11224919\n",
      "epoch 330 loss 0.14413662 lossS 0.22646251 lossY 0.14413662\n",
      "epoch 335 loss 0.1625301 lossS 0.18140127 lossY 0.1625301\n",
      "epoch 340 loss 0.15816782 lossS 0.19746083 lossY 0.15816782\n",
      "epoch 345 loss 0.18662407 lossS 0.15066062 lossY 0.18662407\n",
      "epoch 350 loss 0.20560926 lossS 0.22900248 lossY 0.20560926\n",
      "epoch 355 loss 0.33803242 lossS 0.20595376 lossY 0.33803242\n",
      "epoch 360 loss 0.23208554 lossS 0.3032958 lossY 0.23208554\n",
      "epoch 365 loss 0.14874634 lossS 0.24303448 lossY 0.14874634\n",
      "epoch 370 loss 0.17099051 lossS 0.22849132 lossY 0.17099051\n",
      "epoch 375 loss 0.1885226 lossS 0.21961085 lossY 0.1885226\n",
      "epoch 380 loss 0.21513714 lossS 0.17939918 lossY 0.21513714\n",
      "epoch 385 loss 0.18579756 lossS 0.20799479 lossY 0.18579756\n",
      "epoch 390 loss 0.1708301 lossS 0.250728 lossY 0.1708301\n",
      "epoch 395 loss 0.2048366 lossS 0.20990297 lossY 0.2048366\n",
      "epoch 400 loss 0.21062171 lossS 0.22190094 lossY 0.21062171\n",
      "epoch 405 loss 0.21199654 lossS 0.17125757 lossY 0.21199654\n",
      "epoch 410 loss 0.15668775 lossS 0.23754856 lossY 0.15668775\n",
      "epoch 415 loss 0.2365715 lossS 0.15972045 lossY 0.2365715\n",
      "epoch 420 loss 0.23291044 lossS 0.1768014 lossY 0.23291044\n",
      "epoch 425 loss 0.297475 lossS 0.254208 lossY 0.297475\n",
      "epoch 430 loss 0.16973953 lossS 0.24200553 lossY 0.16973953\n",
      "epoch 435 loss 0.2166472 lossS 0.27648327 lossY 0.2166472\n",
      "epoch 440 loss 0.17755923 lossS 0.2189849 lossY 0.17755923\n",
      "epoch 445 loss 0.2363653 lossS 0.21378659 lossY 0.2363653\n",
      "epoch 450 loss 0.22548872 lossS 0.2300905 lossY 0.22548872\n",
      "epoch 455 loss 0.12484558 lossS 0.27378356 lossY 0.12484558\n",
      "epoch 460 loss 0.22203009 lossS 0.24430099 lossY 0.22203009\n",
      "epoch 465 loss 0.20024335 lossS 0.23468259 lossY 0.20024335\n",
      "epoch 470 loss 0.14065404 lossS 0.22910269 lossY 0.14065404\n",
      "epoch 475 loss 0.20747167 lossS 0.20799625 lossY 0.20747167\n",
      "epoch 480 loss 0.17033102 lossS 0.20297681 lossY 0.17033102\n",
      "epoch 485 loss 0.12691131 lossS 0.21026786 lossY 0.12691131\n",
      "epoch 490 loss 0.1986533 lossS 0.242889 lossY 0.1986533\n",
      "epoch 495 loss 0.22469223 lossS 0.26435965 lossY 0.22469223\n"
     ]
    }
   ],
   "source": [
    "lambda_=0\n",
    "dev= \"cuda:0\"\n",
    "model = ZHANG(learning_rate=0.0001,batch_size= 2048,\n",
    "            lamb= lambda_, num_epochs=500,\n",
    "            NN_y= NN_y, NN_s= NN_s, GPU=dev)\n",
    "model.fit(X_train, y_train, S_train)\n",
    "y_pred = model.predict(X_test)\n",
    "torch.save(model,'NN_biased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d469985",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:40:50.848912Z",
     "start_time": "2024-04-26T21:40:50.707459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coucou\n",
      "{'Global Acc': 0.6415369649805448, 'Global DI': 0.12277215799614644, 'top1_DI': 0.192, 'top3_DI': 0.13533333333333333, 'worst1_acc': 0.59, 'q_DI_0.0': 0.003, 'q_DI_0.1': 0.016300000000000002, 'q_DI_0.2': 0.03560000000000001, 'q_DI_0.30000000000000004': 0.0562, 'q_DI_0.4': 0.057600000000000005, 'q_DI_0.5': 0.0655, 'q_DI_0.6000000000000001': 0.07720000000000002, 'q_DI_0.7000000000000001': 0.09190000000000001, 'q_DI_0.8': 0.10960000000000002, 'q_DI_0.9': 0.14159999999999998, 'Local DI': array([0.073, 0.003, 0.056, 0.094, 0.022, 0.12 , 0.058, 0.192])}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_obs</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_q</th>\n",
       "      <th>Local DI</th>\n",
       "      <th>Local Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>(20, 30]</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>(30, 40]</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>(40, 50]</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>(10, 20]</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>757</td>\n",
       "      <td>1</td>\n",
       "      <td>(20, 30]</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>422</td>\n",
       "      <td>1</td>\n",
       "      <td>(30, 40]</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>243</td>\n",
       "      <td>1</td>\n",
       "      <td>(40, 50]</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>157</td>\n",
       "      <td>1</td>\n",
       "      <td>(50, 60]</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_obs  sex     age_q  Local DI  Local Acc\n",
       "0    157    0  (20, 30]     0.073      0.675\n",
       "1    101    0  (30, 40]     0.003      0.653\n",
       "2     53    0  (40, 50]     0.056      0.698\n",
       "3     63    1  (10, 20]     0.094      0.667\n",
       "4    757    1  (20, 30]     0.022      0.590\n",
       "5    422    1  (30, 40]     0.120      0.618\n",
       "6    243    1  (40, 50]     0.058      0.704\n",
       "7    157    1  (50, 60]     0.192      0.707"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=torch.load('NN_biased')\n",
    "y_pred = model.predict(X_test)\n",
    "gr = get_subroups(X_test0, S_test, continuous_names=[\"age\"], how_continuous='bins', n_min=N_MIN_GROUP, n_q=10,\n",
    "             to_keep=[\"age\"]+[\"sex\"])\n",
    "from utils.fairness_utils import get_subroups_results, get_scores, get_subroups, get_subgroups_random\n",
    "dro_scores, sr = get_scores(gr[0], X_test0, y_test, S_test, y_pred>0.5, n_min=N_MIN_GROUP,\n",
    "              topK_DI=[1, 3], q_DI=10, topK_acc=[1], q_acc=4)\n",
    "print(dro_scores)\n",
    "sr.drop([\"group_label\",\"Acc by S\"], axis=1)[['n_obs','sex','age_q','Local DI','Local Acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5312cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac42f106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:49:53.477337Z",
     "start_time": "2024-04-26T21:49:43.771477Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss -4.016758 lossS 0.1693053 lossY 0.21587396\n",
      "epoch 5 loss -7.6026373 lossS 0.31717092 lossY 0.326636\n",
      "epoch 10 loss -5.8951516 lossS 0.24418959 lossY 0.20958789\n",
      "epoch 15 loss -6.36101 lossS 0.26159754 lossY 0.17892845\n",
      "epoch 20 loss -6.3755274 lossS 0.26330099 lossY 0.20699753\n",
      "epoch 25 loss -6.2820225 lossS 0.261025 lossY 0.24360274\n",
      "epoch 30 loss -4.307335 lossS 0.1800026 lossY 0.19273008\n",
      "epoch 35 loss -4.839108 lossS 0.20340395 lossY 0.24599081\n",
      "epoch 40 loss -5.327918 lossS 0.22258493 lossY 0.23670526\n",
      "epoch 45 loss -4.282429 lossS 0.17916445 lossY 0.19668186\n",
      "epoch 50 loss -5.3968005 lossS 0.22305948 lossY 0.17968646\n",
      "epoch 55 loss -5.8537445 lossS 0.2427783 lossY 0.2157131\n",
      "epoch 60 loss -4.3218575 lossS 0.18141392 lossY 0.21349056\n",
      "epoch 65 loss -3.3488734 lossS 0.14053021 lossY 0.16438189\n",
      "epoch 70 loss -6.887028 lossS 0.28192306 lossY 0.16104852\n",
      "epoch 75 loss -5.8023677 lossS 0.24374677 lossY 0.29130188\n",
      "epoch 80 loss -6.3519835 lossS 0.26073 lossY 0.16626638\n",
      "epoch 85 loss -4.402779 lossS 0.18335019 lossY 0.18097559\n",
      "epoch 90 loss -5.8067904 lossS 0.24128368 lossY 0.22530197\n",
      "epoch 95 loss -6.846315 lossS 0.28082365 lossY 0.17427646\n",
      "epoch 100 loss -5.3464794 lossS 0.22161782 lossY 0.19396597\n",
      "epoch 105 loss -6.4285226 lossS 0.26344284 lossY 0.15754856\n",
      "epoch 110 loss -4.920445 lossS 0.20326701 lossY 0.16123004\n",
      "epoch 115 loss -5.411356 lossS 0.22213534 lossY 0.14202717\n",
      "epoch 120 loss -5.3434443 lossS 0.22240286 lossY 0.21662696\n",
      "epoch 125 loss -6.400554 lossS 0.26232055 lossY 0.15745963\n",
      "epoch 130 loss -5.915087 lossS 0.24384953 lossY 0.18115073\n",
      "epoch 135 loss -5.3627205 lossS 0.22220857 lossY 0.19249392\n",
      "epoch 140 loss -4.2684474 lossS 0.17928989 lossY 0.21379988\n",
      "epoch 145 loss -7.027735 lossS 0.28818005 lossY 0.17676571\n",
      "epoch 150 loss -6.3985834 lossS 0.26443213 lossY 0.21221992\n",
      "epoch 155 loss -6.2969522 lossS 0.26334238 lossY 0.28660738\n",
      "epoch 160 loss -6.319859 lossS 0.26075056 lossY 0.19890516\n",
      "epoch 165 loss -5.9222264 lossS 0.24501646 lossY 0.20318525\n",
      "epoch 170 loss -5.9140997 lossS 0.24392176 lossY 0.18394409\n",
      "epoch 175 loss -5.3484435 lossS 0.22184657 lossY 0.19772032\n",
      "epoch 180 loss -5.3420496 lossS 0.22165586 lossY 0.19934689\n",
      "epoch 185 loss -6.83 lossS 0.2823286 lossY 0.22821528\n",
      "epoch 190 loss -5.8419986 lossS 0.24200475 lossY 0.20812042\n",
      "epoch 195 loss -5.4767227 lossS 0.22431597 lossY 0.13117634\n"
     ]
    }
   ],
   "source": [
    "lambda_=25\n",
    "dev= \"cuda:1\"\n",
    "model = ZHANG(learning_rate=0.0001,batch_size= 512,\n",
    "            lamb= lambda_, num_epochs=200,\n",
    "            NN_y= NN_y, NN_s= NN_s, GPU=dev)\n",
    "model.fit(X_train, y_train, S_train)\n",
    "y_pred = model.predict(X_test)\n",
    "torch.save(model,'ZHANG_'+ str(lambda_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c449af15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T22:06:35.107775Z",
     "start_time": "2024-04-26T22:05:59.816490Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.6921459 lossS 0.5853665 lossY 0.6921459 P-rule -100.0 ACC_train 0.5456093407929944\n",
      "epoch 5 loss 0.68771565 lossS 0.5007448 lossY 0.68771565 P-rule 0.0 ACC_train 0.5460958404281196\n",
      "epoch 10 loss 0.68081224 lossS 0.65163076 lossY 0.68081224 P-rule 53.22400298173686 ACC_train 0.5463390902456823\n",
      "epoch 15 loss 0.6765281 lossS 0.48274234 lossY 0.6765281 P-rule 91.24114796869175 ACC_train 0.5536365847725614\n",
      "epoch 20 loss 0.65217686 lossS 0.56369275 lossY 0.65217686 P-rule 67.50851855893873 ACC_train 0.6159085380685965\n",
      "epoch 25 loss 0.41483182 lossS 0.36046183 lossY 0.41483182 P-rule 69.34737477668233 ACC_train 0.6667477499391875\n",
      "epoch 30 loss 0.5905872 lossS 0.41497466 lossY 0.5905872 P-rule 69.32914699156832 ACC_train 0.681829238628071\n",
      "epoch 35 loss 0.45742843 lossS 0.4080463 lossY 0.45742843 P-rule 69.15642478489768 ACC_train 0.6922889807832644\n",
      "epoch 40 loss 0.7699379 lossS 0.2928624 lossY 0.7699379 P-rule 65.43710041268415 ACC_train 0.6949647287764534\n",
      "epoch 45 loss 0.68645793 lossS 0.2977645 lossY 0.68645793 P-rule 70.03393369877315 ACC_train 0.7017757236682073\n",
      "epoch 50 loss -0.78477883 lossS 0.26795468 lossY 0.5549946 P-rule 67.9770821372027 ACC_train 0.7025054731208952\n",
      "epoch 55 loss -2.1262674 lossS 0.5209102 lossY 0.47828358 P-rule 68.89122315592904 ACC_train 0.703964972026271\n",
      "epoch 60 loss -1.7039576 lossS 0.42304358 lossY 0.4112603 P-rule 71.73492055162832 ACC_train 0.7051812211140842\n",
      "epoch 65 loss -2.4495268 lossS 0.5847687 lossY 0.47431692 P-rule 75.26340313040832 ACC_train 0.7029919727560204\n",
      "epoch 70 loss -1.1688049 lossS 0.35379896 lossY 0.60019 P-rule 79.22136544440069 ACC_train 0.7059109705667721\n",
      "epoch 75 loss -2.0518835 lossS 0.50178576 lossY 0.45704532 P-rule 78.41559523162913 ACC_train 0.7037217222087083\n",
      "epoch 80 loss -2.3560758 lossS 0.5974422 lossY 0.6311353 P-rule 80.1704875234287 ACC_train 0.7049379712965216\n",
      "epoch 85 loss -2.1320822 lossS 0.5155306 lossY 0.44557077 P-rule 82.11647247551502 ACC_train 0.7042082218438336\n",
      "epoch 90 loss -1.9995189 lossS 0.48266548 lossY 0.41380858 P-rule 81.4109749860209 ACC_train 0.7027487229384578\n",
      "epoch 95 loss -1.9819348 lossS 0.5005136 lossY 0.5206332 P-rule 82.15370461910254 ACC_train 0.7000729749452688\n"
     ]
    }
   ],
   "source": [
    "lambda_=5\n",
    "modelADEL = ADEL(learning_rate=0.0001,batch_size= 1024,\n",
    "            lamb= lambda_, num_epochs=100,\n",
    "           NN_y= NN_y_adel, NN_s= NN_s_adel, NN_z=NN_z_adel, GPU=dev)\n",
    "modelADEL.fit(X_train, y_train, S_train)\n",
    "y_pred = modelADEL.predict(X_test)\n",
    "torch.save(modelADEL,'ADEL_'+ str(lambda_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "797af481",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T22:07:31.152853Z",
     "start_time": "2024-04-26T22:07:31.057994Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coucou\n",
      "{'Global Acc': 0.6099221789883269, 'Global DI': 0.036935728048444805, 'top1_DI': 0.201, 'top3_DI': 0.128, 'worst1_acc': 0.524, 'q_DI_0.0': 0.001, 'q_DI_0.1': 0.011500000000000002, 'q_DI_0.2': 0.0184, 'q_DI_0.30000000000000004': 0.022500000000000003, 'q_DI_0.4': 0.026000000000000002, 'q_DI_0.5': 0.028999999999999998, 'q_DI_0.6000000000000001': 0.03740000000000003, 'q_DI_0.7000000000000001': 0.05980000000000001, 'q_DI_0.8': 0.09720000000000004, 'q_DI_0.9': 0.14429999999999998, 'Local DI': array([0.201, 0.027, 0.063, 0.031, 0.001, 0.016, 0.022, 0.12 ])}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_obs</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_q</th>\n",
       "      <th>Local DI</th>\n",
       "      <th>Local Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>(20, 30]</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>(30, 40]</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>(40, 50]</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>(10, 20]</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>757</td>\n",
       "      <td>1</td>\n",
       "      <td>(20, 30]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>422</td>\n",
       "      <td>1</td>\n",
       "      <td>(30, 40]</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>243</td>\n",
       "      <td>1</td>\n",
       "      <td>(40, 50]</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>157</td>\n",
       "      <td>1</td>\n",
       "      <td>(50, 60]</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_obs  sex     age_q  Local DI  Local Acc\n",
       "0    157    0  (20, 30]     0.201      0.637\n",
       "1    101    0  (30, 40]     0.027      0.683\n",
       "2     53    0  (40, 50]     0.063      0.698\n",
       "3     63    1  (10, 20]     0.031      0.524\n",
       "4    757    1  (20, 30]     0.001      0.572\n",
       "5    422    1  (30, 40]     0.016      0.583\n",
       "6    243    1  (40, 50]     0.022      0.654\n",
       "7    157    1  (50, 60]     0.120      0.669"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model=torch.load('NN_biased')\n",
    "#model=torch.load('ZHANG_20')\n",
    "#model=torch.load('ZHANG_25')\n",
    "model=torch.load('ADEL_5')\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "gr = get_subroups(X_test0, S_test, continuous_names=[\"age\"], how_continuous='bins', n_min=N_MIN_GROUP, n_q=10,\n",
    "             to_keep=[\"age\"]+[\"sex\"])\n",
    "from utils.fairness_utils import get_subroups_results, get_scores, get_subroups, get_subgroups_random\n",
    "dro_scores, sr = get_scores(gr[0], X_test0, y_test, S_test, y_pred>0.5, n_min=N_MIN_GROUP,\n",
    "              topK_DI=[1, 3], q_DI=10, topK_acc=[1], q_acc=4)\n",
    "print(dro_scores)\n",
    "sr.drop([\"group_label\",\"Acc by S\"], axis=1)[['n_obs','sex','age_q','Local DI','Local Acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e1087e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T23:41:08.686365Z",
     "start_time": "2024-04-26T23:41:08.656228Z"
    }
   },
   "outputs": [],
   "source": [
    "import fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1ed4d4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T23:47:19.117073Z",
     "start_time": "2024-04-22T23:47:19.085581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 30], (40, 50], (50, 60], (30, 40], (10, 20], (60, 70], (70, 80]]\n",
       "Categories (8, interval[int64, right]): [(0, 10] < (10, 20] < (20, 30] < (30, 40] < (40, 50] < (50, 60] < (60, 70] < (70, 80]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gr[0]['age_q'].unique()\n",
    "#gr_rdm = get_subgroups_random(X_test0, S_test, n_groups=20)\n",
    "#dro_scores_rdm = get_scores(gr_rdm[0], X_test0, y_test, S_test, y_pred>0.5, n_min=N_MIN_GROUP,\n",
    "#          topK_DI=[1, 3], q_DI=10, topK_acc=[1], q_acc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2abb0ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T23:41:13.907109Z",
     "start_time": "2024-04-26T23:41:13.878711Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def DATA_TRAIN_TEST2(num,sens,y,columns_delete,scaling):\n",
    "    dataset = DATASETS[num] # Adult data set\n",
    "    all_sensitive_attributes = dataset.get_sensitive_attributes_with_joint()\n",
    "    ProcessedData(dataset)\n",
    "    processed_dataset = ProcessedData(dataset)\n",
    "    train_test_splits = processed_dataset.create_train_test_splits(1)\n",
    "    train_test_splits.keys()\n",
    "    train, test = train_test_splits['numerical-binsensitive'][0]\n",
    "    #all_sensitive_attributes = dataset.get_sensitive_attributes_with_joint()\n",
    "    #for supported_tag in algorithm.get_supported_data_types():\n",
    "    supported_tag = 'numerical-binsensitive'\n",
    "    privileged_vals = dataset.get_privileged_class_names_with_joint(supported_tag)\n",
    "    positive_val = dataset.get_positive_class_val(supported_tag)\n",
    "    sensitive = sens\n",
    "    print(\"===================================\")\n",
    "    print(\"supported tag: \" + supported_tag)\n",
    "    print(\"sensitive attribute: \" + sensitive)\n",
    "    print(\"===================================\")\n",
    "    if scaling :\n",
    "        scaler = StandardScaler().fit(train)\n",
    "        s=train[sens]\n",
    "        st=test[sens]\n",
    "        t=train[y]\n",
    "        tt=test[y]\n",
    "        scale_df = lambda df, scaler: pd.DataFrame(scaler.transform(df), columns=df.columns, index=df.index)\n",
    "        train = train.pipe(scale_df, scaler)\n",
    "        test = test.pipe(scale_df, scaler)\n",
    "        train= train.drop(columns_delete, axis=1)\n",
    "        train[sens] = s\n",
    "        train[y] = t\n",
    "        test= test.drop(columns_delete, axis=1)\n",
    "        test[sens] = st\n",
    "        test[y] = tt\n",
    "\n",
    "    return train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfc5535d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T23:41:19.526276Z",
     "start_time": "2024-04-26T23:41:19.488620Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available algorithms:\n",
      "  SVM\n",
      "  GaussianNB\n",
      "  LR\n",
      "  DecisionTree\n",
      "  Kamishima\n",
      "  Calders\n",
      "  ZafarBaseline\n",
      "  ZafarFairness\n",
      "  ZafarAccuracy\n",
      "  Kamishima-accuracy\n",
      "  Kamishima-DIavgall\n",
      "  Feldman-SVM\n",
      "  Feldman-GaussianNB\n",
      "  Feldman-LR\n",
      "  Feldman-DecisionTree\n",
      "  Feldman-SVM-DIavgall\n",
      "  Feldman-SVM-accuracy\n",
      "  Feldman-GaussianNB-DIavgall\n",
      "  Feldman-GaussianNB-accuracy\n"
     ]
    }
   ],
   "source": [
    "from fairness.algorithms.zafar.ZafarAlgorithm import ZafarAlgorithmBaseline, ZafarAlgorithmAccuracy, ZafarAlgorithmFairness\n",
    "from fairness.algorithms.kamishima.KamishimaAlgorithm import KamishimaAlgorithm\n",
    "from fairness.algorithms.kamishima.CaldersAlgorithm import CaldersAlgorithm\n",
    "from fairness.algorithms.feldman.FeldmanAlgorithm import FeldmanAlgorithm\n",
    "from fairness.algorithms.baseline.SVM import SVM\n",
    "from fairness.algorithms.baseline.DecisionTree import DecisionTree\n",
    "from fairness.algorithms.baseline.GaussianNB import GaussianNB\n",
    "from fairness.algorithms.baseline.LogisticRegression import LogisticRegression\n",
    "from fairness.algorithms.ParamGridSearch import ParamGridSearch\n",
    "from fairness.algorithms.Ben.SDBSVM import SDBSVM\n",
    "\n",
    "\n",
    "def display_results(y_pred, y, sensitive):\n",
    "    y_pred2 = (y_pred>0.5).astype(int)\n",
    "    accuracy = accuracy_score(y, np.squeeze(y_pred2))\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"PRULE : \", p_rule(y_pred,sensitive))\n",
    "    print(\"DI : \", DI(y_pred, sensitive))\n",
    "    print(\"DispFPR : \", DispFPR(y_pred2, y, sensitive))\n",
    "    print(\"DispFNR : \", DispFNR(y_pred2, y, sensitive))\n",
    "    return {'Accuracy': accuracy, 'PRULE': p_rule(y_pred,sensitive), 'DispFPR': DispFPR(y_pred2, y, sensitive)\n",
    "            ,'DispFNR': DispFNR(y_pred2, y, sensitive)}\n",
    "\n",
    "def p_rule(y_pred, z_values, threshold=0.5):\n",
    "    \n",
    "    y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n",
    "    y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\n",
    "    odds = y_z_1.mean() / y_z_0.mean()\n",
    "    return np.min([odds, 1/odds]) * 100\n",
    "\n",
    "def DI(y_pred, z_values, threshold=0.5):\n",
    "    y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n",
    "    y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\n",
    "    odds = abs(y_z_1.mean() - y_z_0.mean())\n",
    "    return odds\n",
    "   \n",
    "def DispFNR(y_pred, y, z_values, threshold=0.5):\n",
    "    ypred_z_1 = y_pred > threshold if threshold else y_pred[z_values == 1]\n",
    "    ypred_z_0 = y_pred > threshold if threshold else y_pred[z_values == 0]\n",
    "    result=abs(ypred_z_1[(y==1) & (z_values==0)].mean()-ypred_z_1[(y==1) & (z_values==1)].mean())\n",
    "    return result\n",
    "def DispFPR(y_pred, y, z_values, threshold=0.5):\n",
    "    ypred_z_1 = y_pred > threshold if threshold else y_pred[z_values == 1]\n",
    "    ypred_z_0 = y_pred > threshold if threshold else y_pred[z_values == 0]\n",
    "    result=abs(ypred_z_1[(y==0) & (z_values==0)].mean()-ypred_z_1[(y==0) & (z_values==1)].mean())\n",
    "    return result\n",
    "\n",
    "\n",
    "from fairness.data.objects.list import DATASETS, get_dataset_names\n",
    "from fairness.data.objects.ProcessedData import ProcessedData\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from fairness.benchmark import run_alg\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "38f1eb27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T23:16:45.661325Z",
     "start_time": "2024-04-26T23:16:39.027920Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6152723735408561\n",
      "PRULE :  93.90251144510668\n",
      "DI :  0.024123881991723384\n",
      "DispFPR :  0.010697177351893739\n",
      "DispFNR :  0.011450381679389277\n",
      "coucou\n",
      "{'Global Acc': 0.6152723735408561, 'Global DI': 0.024123881991723384, 'top1_DI': 0.164, 'top3_DI': 0.15166666666666667, 'worst1_acc': 0.579, 'q_DI_0.0': 0.029, 'q_DI_0.1': 0.0346, 'q_DI_0.2': 0.0398, 'q_DI_0.30000000000000004': 0.0449, 'q_DI_0.4': 0.0512, 'q_DI_0.5': 0.069, 'q_DI_0.6000000000000001': 0.09420000000000006, 'q_DI_0.7000000000000001': 0.1264, 'q_DI_0.8': 0.14840000000000003, 'q_DI_0.9': 0.1612, 'Local DI': array([0.16 , 0.164, 0.037, 0.053, 0.085, 0.044, 0.131, 0.029])}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_obs</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_q</th>\n",
       "      <th>Local DI</th>\n",
       "      <th>Local Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>(20, 30]</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>(30, 40]</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>(40, 50]</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>(10, 20]</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>775</td>\n",
       "      <td>1</td>\n",
       "      <td>(20, 30]</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>391</td>\n",
       "      <td>1</td>\n",
       "      <td>(30, 40]</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "      <td>(40, 50]</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>(50, 60]</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_obs  sex     age_q  Local DI  Local Acc\n",
       "0    165    0  (20, 30]     0.160      0.606\n",
       "1    107    0  (30, 40]     0.164      0.607\n",
       "2     54    0  (40, 50]     0.037      0.648\n",
       "3     52    1  (10, 20]     0.053      0.712\n",
       "4    775    1  (20, 30]     0.085      0.579\n",
       "5    391    1  (30, 40]     0.044      0.673\n",
       "6    235    1  (40, 50]     0.131      0.600\n",
       "7    162    1  (50, 60]     0.029      0.636"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "algorithm = FeldmanAlgorithm(DecisionTree())  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, ycible, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = ycible\n",
    "    params = {'lambda': 0.8} \n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "\n",
    "table = [0,0,0,0]\n",
    "\n",
    "#a, b, c, d, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "sensitive='race'\n",
    "X_train_temp =  pd.DataFrame(X_train)\n",
    "X_train_temp['two_year_recid'] = y_train.values\n",
    "X_train_temp['race']= S_train\n",
    "\n",
    "X_test_temp =  pd.DataFrame(X_test)\n",
    "X_test_temp['two_year_recid'] = y_test.values\n",
    "X_test_temp['race']= S_test\n",
    "all_sensitive_attributes = ['race']\n",
    "ycible = 'two_year_recid'\n",
    "positive_val=1\n",
    "privileged_vals=[1, 1, '1-1']\n",
    "predicted, params, predictions_list = run_alg(algorithm, X_train_temp, X_test_temp, ycible, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "actual = X_test_temp['two_year_recid'].values\n",
    "y_pred = np.asarray(predicted)\n",
    "z_values = X_test_temp[sensitive].values\n",
    "Res = display_results(y_pred, actual, z_values)\n",
    "table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "#np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "gr = get_subroups(X_test0, S_test, continuous_names=[\"age\"], how_continuous='bins', n_min=N_MIN_GROUP, n_q=10,\n",
    "             to_keep=[\"age\"]+[\"sex\"])\n",
    "from utils.fairness_utils import get_subroups_results, get_scores, get_subroups, get_subgroups_random\n",
    "dro_scores, sr = get_scores(gr[0], X_test0, y_test, S_test, y_pred>0.5, n_min=N_MIN_GROUP,\n",
    "              topK_DI=[1, 3], q_DI=10, topK_acc=[1], q_acc=4)\n",
    "print(dro_scores)\n",
    "sr.drop([\"group_label\",\"Acc by S\"], axis=1)[['n_obs','sex','age_q','Local DI','Local Acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "84d631c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6425097276264592\n",
      "PRULE :  83.56157226374522\n",
      "DI :  0.06384087571752767\n",
      "DispFPR :  0.04795121568145422\n",
      "DispFNR :  0.15249107909119658\n",
      "coucou\n",
      "{'Global Acc': 0.6425097276264592, 'Global DI': 0.06384087571752767, 'top1_DI': 0.537, 'top3_DI': 0.30633333333333335, 'worst1_acc': 0.519, 'q_DI_0.0': 0.045, 'q_DI_0.1': 0.06319999999999999, 'q_DI_0.2': 0.071, 'q_DI_0.30000000000000004': 0.07480000000000002, 'q_DI_0.4': 0.1014, 'q_DI_0.5': 0.11449999999999999, 'q_DI_0.6000000000000001': 0.1206, 'q_DI_0.7000000000000001': 0.1227, 'q_DI_0.8': 0.20460000000000006, 'q_DI_0.9': 0.3424, 'Local DI': array([0.071, 0.071, 0.259, 0.537, 0.123, 0.12 , 0.045, 0.109])}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_obs</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_q</th>\n",
       "      <th>Local DI</th>\n",
       "      <th>Local Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>(20, 30]</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>(30, 40]</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>(40, 50]</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>(10, 20]</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>775</td>\n",
       "      <td>1</td>\n",
       "      <td>(20, 30]</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>391</td>\n",
       "      <td>1</td>\n",
       "      <td>(30, 40]</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>235</td>\n",
       "      <td>1</td>\n",
       "      <td>(40, 50]</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>(50, 60]</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_obs  sex     age_q  Local DI  Local Acc\n",
       "0    165    0  (20, 30]     0.071      0.636\n",
       "1    107    0  (30, 40]     0.071      0.664\n",
       "2     54    0  (40, 50]     0.259      0.722\n",
       "3     52    1  (10, 20]     0.537      0.519\n",
       "4    775    1  (20, 30]     0.123      0.619\n",
       "5    391    1  (30, 40]     0.120      0.675\n",
       "6    235    1  (40, 50]     0.045      0.617\n",
       "7    162    1  (50, 60]     0.109      0.673"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "algorithm = KamishimaAlgorithm()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, ycible, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = ycible\n",
    "    params = {'eta': 100000} \n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "\n",
    "table = [0,0,0,0]\n",
    "\n",
    "#a, b, c, d, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "sensitive='race'\n",
    "X_train_temp =  pd.DataFrame(X_train)\n",
    "X_train_temp['two_year_recid'] = y_train.values\n",
    "X_train_temp['race']= S_train\n",
    "\n",
    "X_test_temp =  pd.DataFrame(X_test)\n",
    "X_test_temp['two_year_recid'] = y_test.values\n",
    "X_test_temp['race']= S_test\n",
    "all_sensitive_attributes = ['race']\n",
    "ycible = 'two_year_recid'\n",
    "positive_val=1\n",
    "privileged_vals=[1, 1, '1-1']\n",
    "predicted, params, predictions_list = run_alg(algorithm, X_train_temp, X_test_temp, ycible, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "actual = X_test_temp['two_year_recid'].values\n",
    "y_pred = np.asarray(predicted)\n",
    "z_values = X_test_temp[sensitive].values\n",
    "Res = display_results(y_pred, actual, z_values)\n",
    "table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "#np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "gr = get_subroups(X_test0, S_test, continuous_names=[\"age\"], how_continuous='bins', n_min=N_MIN_GROUP, n_q=10,\n",
    "             to_keep=[\"age\"]+[\"sex\"])\n",
    "from utils.fairness_utils import get_subroups_results, get_scores, get_subroups, get_subgroups_random\n",
    "dro_scores, sr = get_scores(gr[0], X_test0, y_test, S_test, y_pred>0.5, n_min=N_MIN_GROUP,\n",
    "              topK_DI=[1, 3], q_DI=10, topK_acc=[1], q_acc=4)\n",
    "print(dro_scores)\n",
    "sr.drop([\"group_label\",\"Acc by S\"], axis=1)[['n_obs','sex','age_q','Local DI','Local Acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "558f4b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6702334630350194\n",
      "PRULE :  61.29721926236552\n",
      "DI :  0.17732806274312168\n",
      "DispFPR :  0.11547182975754403\n",
      "DispFNR :  0.18701848998459164\n",
      "coucou\n",
      "{'Global Acc': 0.6702334630350194, 'Global DI': 0.17732806274312168, 'top1_DI': 0.165, 'top3_DI': 0.14233333333333334, 'worst1_acc': 0.61, 'q_DI_0.0': 0.005, 'q_DI_0.1': 0.015500000000000002, 'q_DI_0.2': 0.027600000000000003, 'q_DI_0.30000000000000004': 0.04390000000000002, 'q_DI_0.4': 0.0782, 'q_DI_0.5': 0.0925, 'q_DI_0.6000000000000001': 0.10200000000000004, 'q_DI_0.7000000000000001': 0.11950000000000001, 'q_DI_0.8': 0.13280000000000003, 'q_DI_0.9': 0.14750000000000002, 'Local DI': array([0.039, 0.005, 0.088, 0.02 , 0.122, 0.097, 0.165, 0.14 ])}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_obs</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_q</th>\n",
       "      <th>Local DI</th>\n",
       "      <th>Local Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>(20, 30]</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>(30, 40]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>(40, 50]</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>(10, 20]</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>732</td>\n",
       "      <td>1</td>\n",
       "      <td>(20, 30]</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>404</td>\n",
       "      <td>1</td>\n",
       "      <td>(30, 40]</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>252</td>\n",
       "      <td>1</td>\n",
       "      <td>(40, 50]</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>(50, 60]</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_obs  sex     age_q  Local DI  Local Acc\n",
       "0    163    0  (20, 30]     0.039      0.687\n",
       "1    113    0  (30, 40]     0.005      0.664\n",
       "2     56    0  (40, 50]     0.088      0.696\n",
       "3     59    1  (10, 20]     0.020      0.610\n",
       "4    732    1  (20, 30]     0.122      0.634\n",
       "5    404    1  (30, 40]     0.097      0.634\n",
       "6    252    1  (40, 50]     0.165      0.758\n",
       "7    153    1  (50, 60]     0.140      0.739"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.ZafarAlgorithm import ZafarAlgorithmFairnessNEW\n",
    "algorithm = ZafarAlgorithmFairnessNEW()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, ycible, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = ycible\n",
    "    params = {'c': 100} \n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "\n",
    "table = [0,0,0,0]\n",
    "\n",
    "#a, b, c, d, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "sensitive='race'\n",
    "X_train_temp =  pd.DataFrame(X_train)\n",
    "X_train_temp['two_year_recid'] = y_train.values\n",
    "X_train_temp['race']= S_train\n",
    "\n",
    "X_test_temp =  pd.DataFrame(X_test)\n",
    "X_test_temp['two_year_recid'] = y_test.values\n",
    "X_test_temp['race']= S_test\n",
    "all_sensitive_attributes = ['race']\n",
    "ycible = 'two_year_recid'\n",
    "positive_val=1\n",
    "privileged_vals=[1, 1, '1-1']\n",
    "predicted, params, predictions_list = run_alg(algorithm, X_train_temp, X_test_temp, ycible, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "actual = X_test_temp['two_year_recid'].values\n",
    "y_pred = np.asarray(predicted)\n",
    "z_values = X_test_temp[sensitive].values\n",
    "Res = display_results(y_pred, actual, z_values)\n",
    "table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "#np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "gr = get_subroups(X_test0, S_test, continuous_names=[\"age\"], how_continuous='bins', n_min=N_MIN_GROUP, n_q=10,\n",
    "             to_keep=[\"age\"]+[\"sex\"])\n",
    "from utils.fairness_utils import get_subroups_results, get_scores, get_subroups, get_subgroups_random\n",
    "dro_scores, sr = get_scores(gr[0], X_test0, y_test, S_test, y_pred>0.5, n_min=N_MIN_GROUP,\n",
    "              topK_DI=[1, 3], q_DI=10, topK_acc=[1], q_acc=4)\n",
    "print(dro_scores)\n",
    "sr.drop([\"group_label\",\"Acc by S\"], axis=1)[['n_obs','sex','age_q','Local DI','Local Acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fdc8384",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=X_train_temp.drop(columns=['two_year_recid']).to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25edadbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {}\n",
    "out[\"x\"] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9e01fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eac33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "algorithm = CaldersAlgorithm()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, ycible, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = ycible\n",
    "    params = {'beta': 0.2} \n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "\n",
    "table = [0,0,0,0]\n",
    "\n",
    "#a, b, c, d, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "sensitive='race'\n",
    "X_train_temp =  pd.DataFrame(X_train)\n",
    "X_train_temp['two_year_recid'] = y_train.values\n",
    "X_train_temp['race']= S_train\n",
    "\n",
    "X_test_temp =  pd.DataFrame(X_test)\n",
    "X_test_temp['two_year_recid'] = y_test.values\n",
    "X_test_temp['race']= S_test\n",
    "all_sensitive_attributes = ['race']\n",
    "ycible = 'two_year_recid'\n",
    "positive_val=1\n",
    "privileged_vals=[1, 1, '1-1']\n",
    "predicted, params, predictions_list = run_alg(algorithm, X_train_temp, X_test_temp, ycible, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "actual = X_test_temp['two_year_recid'].values\n",
    "y_pred = np.asarray(predicted)\n",
    "z_values = X_test_temp[sensitive].values\n",
    "Res = display_results(y_pred, actual, z_values)\n",
    "table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "#np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "gr = get_subroups(X_test0, S_test, continuous_names=[\"age\"], how_continuous='bins', n_min=N_MIN_GROUP, n_q=10,\n",
    "             to_keep=[\"age\"]+[\"sex\"])\n",
    "from utils.fairness_utils import get_subroups_results, get_scores, get_subroups, get_subgroups_random\n",
    "dro_scores, sr = get_scores(gr[0], X_test0, y_test, S_test, y_pred>0.5, n_min=N_MIN_GROUP,\n",
    "              topK_DI=[1, 3], q_DI=10, topK_acc=[1], q_acc=4)\n",
    "print(dro_scores)\n",
    "sr.drop([\"group_label\",\"Acc by S\"], axis=1)[['n_obs','sex','age_q','Local DI','Local Acc']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4099a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7858698a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T23:41:35.688396Z",
     "start_time": "2024-04-26T23:41:33.539676Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "supported tag: numerical-binsensitive\n",
      "sensitive attribute: sex\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/train_cv2nb.py\", line 254, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/train_cv2nb.py\", line 128, in main\n",
      "    clr.fit(X, y, ns)\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/fadm/nb/cv2nb.py\", line 155, in fit\n",
      "    numpos, disc = self._get_stats(X, y)\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/fadm/nb/cv2nb.py\", line 170, in _get_stats\n",
      "    py = self.predict(X)\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/fadm/nb/_nb.py\", line 62, in predict\n",
      "    log_proba = self._predict_log_proba_upto_const(np.array(X))\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/fadm/nb/cv2nb.py\", line 200, in _predict_log_proba_upto_const\n",
      "    XX[s == si, :]) + \\\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/fadm/nb/_nb.py\", line 637, in _predict_composite_log_proba_upto_const\n",
      "    self._predict_multinomial_log_proba_upto_const(X[i, self.mfeatures])\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/fadm/nb/_nb.py\", line 502, in _predict_multinomial_log_proba_upto_const\n",
      "    log_proba = np.sum([p(i) for i in f], axis=0)\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/fadm/nb/_nb.py\", line 502, in <listcomp>\n",
      "    log_proba = np.sum([p(i) for i in f], axis=0)\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/fadm/nb/_nb.py\", line 501, in <lambda>\n",
      "    - np.log(np.sum(self.pf_[i], axis=1))\n",
      "  File \"<__array_function__ internals>\", line 6, in sum\n",
      "  File \"/home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages/numpy/core/fromnumeric.py\", line 2260, in sum\n",
      "    initial=initial, where=where)\n",
      "  File \"/home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages/numpy/core/fromnumeric.py\", line 70, in _wrapreduction\n",
      "    passkwargs = {k: v for k, v in kwargs.items()\n",
      "  File \"/home/vince/anaconda3/envs/fair_env/lib/python3.7/site-packages/numpy/core/fromnumeric.py\", line 71, in <dictcomp>\n",
      "    if v is not np._NoValue}\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     15\u001b[0m     train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val \u001b[38;5;241m=\u001b[39m DATA_TRAIN_TEST2(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincome-per-year\u001b[39m\u001b[38;5;124m\"\u001b[39m,[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincome-per-year\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m     predicted, params, predictions_list \u001b[38;5;241m=\u001b[39m \u001b[43mrun_alg\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_sensitive_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivileged_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     actual \u001b[38;5;241m=\u001b[39m test[dataset\u001b[38;5;241m.\u001b[39mget_class_attribute()]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     18\u001b[0m     y_pred2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(predicted)\n",
      "Cell \u001b[0;32mIn[47], line 10\u001b[0m, in \u001b[0;36mrun_alg\u001b[0;34m(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive, privileged_vals, positive_val)\u001b[0m\n\u001b[1;32m      7\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1\u001b[39m} \u001b[38;5;66;03m# DP\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#params = {'eta': 3} # EODDS\u001b[39;00m\n\u001b[1;32m      9\u001b[0m predictions, predictions_list \u001b[38;5;241m=\u001b[39m  \\\n\u001b[0;32m---> 10\u001b[0m     \u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_sensitive_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                  \u001b[49m\u001b[43msingle_sensitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivileged_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions, params, predictions_list\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/CaldersAlgorithm.py:97\u001b[0m, in \u001b[0;36mCaldersAlgorithm.run\u001b[0;34m(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals, params)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# print(\"WILL RUN: %s\" % cmdline)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     result1 \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmdline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result1\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining procedure failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/subprocess.py:495\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    497\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/subprocess.py:1028\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1028\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/subprocess.py:1894\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1891\u001b[0m                     key\u001b[38;5;241m.\u001b[39mfileobj\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   1892\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj2output[key\u001b[38;5;241m.\u001b[39mfileobj]\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m-> 1894\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remaining_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1896\u001b[0m \u001b[38;5;66;03m# All data exchanged.  Translate lists into strings.\u001b[39;00m\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/subprocess.py:1083\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/subprocess.py:1800\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1798\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, timeout)\n\u001b[1;32m   1799\u001b[0m         delay \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(delay \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, remaining, \u001b[38;5;241m.05\u001b[39m)\n\u001b[0;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1802\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#####################   ADULT  #####################\n",
    "\n",
    "algorithm = CaldersAlgorithm()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'beta': 0.1} # DP\n",
    "    #params = {'eta': 3} # EODDS\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['sex','income-per-year'],False)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a8698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ebf2b5c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/zafar/ZafarAlgorithm.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m~/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/zafar/ZafarAlgorithm.py\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# read a list of lines into data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     data \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/zafar/ZafarAlgorithm.py'"
     ]
    }
   ],
   "source": [
    "with open('~/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/zafar/ZafarAlgorithm.py', 'r') as file:\n",
    "    # read a list of lines into data\n",
    "    data = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "63c9ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness.algorithms.zafar.ZafarAlgorithm import ZafarAlgorithmBaseline, ZafarAlgorithmAccuracy, ZafarAlgorithmFairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8f1710aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ZafarAlgorithm import ZafarAlgorithmFairnessNEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7f4b2da8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T23:33:04.966086Z",
     "start_time": "2024-04-26T23:33:04.514287Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "supported tag: numerical-binsensitive\n",
      "sensitive attribute: race\n",
      "===================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     13\u001b[0m     train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val \u001b[38;5;241m=\u001b[39m DATA_TRAIN_TEST2(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m,[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m     predicted, params, predictions_list \u001b[38;5;241m=\u001b[39m \u001b[43mrun_alg\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_sensitive_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivileged_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     actual \u001b[38;5;241m=\u001b[39m test[dataset\u001b[38;5;241m.\u001b[39mget_class_attribute()]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     16\u001b[0m     y_pred2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(predicted)\n",
      "Cell \u001b[0;32mIn[102], line 8\u001b[0m, in \u001b[0;36mrun_alg\u001b[0;34m(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive, privileged_vals, positive_val)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#params = {'c': 10} \u001b[39;00m\n\u001b[1;32m      6\u001b[0m params \u001b[38;5;241m=\u001b[39m algorithm\u001b[38;5;241m.\u001b[39mget_default_params()\n\u001b[1;32m      7\u001b[0m predictions, predictions_list \u001b[38;5;241m=\u001b[39m  \\\n\u001b[0;32m----> 8\u001b[0m     \u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_sensitive_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                  \u001b[49m\u001b[43msingle_sensitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivileged_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions, params, predictions_list\n",
      "File \u001b[0;32m~/FairUnfair/notebooks/models/ZafarAlgorithm.py:40\u001b[0m, in \u001b[0;36mZafarAlgorithmBase.run\u001b[0;34m(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals, params)\u001b[0m\n\u001b[1;32m     37\u001b[0m     out_file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m name\n\u001b[0;32m---> 40\u001b[0m train_name \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m test_name \u001b[38;5;241m=\u001b[39m create_file(test_df)\n\u001b[1;32m     42\u001b[0m fd, predictions_name \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mmkstemp()\n",
      "File \u001b[0;32m~/FairUnfair/notebooks/models/ZafarAlgorithm.py:29\u001b[0m, in \u001b[0;36mZafarAlgorithmBase.run.<locals>.create_file\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_file\u001b[39m(df):\n\u001b[1;32m     28\u001b[0m     out \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 29\u001b[0m     out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mclass_attr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     30\u001b[0m     out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m df[class_attr] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     31\u001b[0m     out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msensitive\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "algorithm = ZafarAlgorithmFairnessNEW()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    #params = {'c': 10} \n",
    "    params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89593cfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T23:28:10.002657Z",
     "start_time": "2024-04-26T23:28:09.923145Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (402, 4111) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m positive_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     26\u001b[0m privileged_vals\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1-1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 27\u001b[0m predicted, params, predictions_list \u001b[38;5;241m=\u001b[39m \u001b[43mrun_alg\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mycible\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_sensitive_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivileged_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m actual \u001b[38;5;241m=\u001b[39m X_test_temp[dataset\u001b[38;5;241m.\u001b[39mget_class_attribute()]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     29\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(predicted)\n",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m, in \u001b[0;36mrun_alg\u001b[0;34m(algorithm, train, test, ycible, all_sensitive_attributes, single_sensitive, privileged_vals, positive_val)\u001b[0m\n\u001b[1;32m      5\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m} \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#params = algorithm.get_default_params()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m predictions, predictions_list \u001b[38;5;241m=\u001b[39m  \\\n\u001b[0;32m----> 8\u001b[0m     \u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_sensitive_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                  \u001b[49m\u001b[43msingle_sensitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivileged_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions, params, predictions_list\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/KamishimaAlgorithm.py:80\u001b[0m, in \u001b[0;36mKamishimaAlgorithm.run\u001b[0;34m(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals, params)\u001b[0m\n\u001b[1;32m     78\u001b[0m fd, output_name \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mmkstemp()\n\u001b[1;32m     79\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(fd)\n\u001b[0;32m---> 80\u001b[0m train_name \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_file_in_kamishima_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m test_name \u001b[38;5;241m=\u001b[39m create_file_in_kamishima_format(test_df)\n\u001b[1;32m     82\u001b[0m eta_val \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/KamishimaAlgorithm.py:70\u001b[0m, in \u001b[0;36mKamishimaAlgorithm.run.<locals>.create_file_in_kamishima_format\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     67\u001b[0m x\u001b[38;5;241m.\u001b[39mappend(numpy\u001b[38;5;241m.\u001b[39marray(s, dtype\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mfloat64))\n\u001b[1;32m     68\u001b[0m x\u001b[38;5;241m.\u001b[39mappend(numpy\u001b[38;5;241m.\u001b[39marray(df[class_attr], dtype\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mfloat64))\n\u001b[0;32m---> 70\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     71\u001b[0m fd, name \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mmkstemp()\n\u001b[1;32m     72\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(fd)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (402, 4111) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "\n",
    "algorithm = KamishimaAlgorithm()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, ycible, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = ycible\n",
    "    params = {'eta': 5} \n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "\n",
    "table = [0,0,0,0]\n",
    "\n",
    "#a, b, c, d, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "sensitive='race'\n",
    "X_train_temp =  X_train0\n",
    "X_train_temp['two_year_recid'] = y_train\n",
    "X_train_temp['race']= S_train\n",
    "\n",
    "X_test_temp =  X_test0\n",
    "X_test_temp['two_year_recid'] = y_test\n",
    "X_test_temp['race']= S_test\n",
    "all_sensitive_attributes = ['sex', 'race']\n",
    "ycible = ['two_year_recid']\n",
    "positive_val=1\n",
    "privileged_vals=[1, 1, '1-1']\n",
    "predicted, params, predictions_list = run_alg(algorithm, X_train_temp, X_test_temp, ycible, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "actual = X_test_temp[dataset.get_class_attribute()].values\n",
    "y_pred = np.asarray(predicted)\n",
    "z_values = X_test_temp[sensitive].values\n",
    "Res = display_results(y_pred, actual, z_values)\n",
    "table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "#np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "gr = get_subroups(X_test0, S_test, continuous_names=[\"age\"], how_continuous='bins', n_min=N_MIN_GROUP, n_q=10,\n",
    "             to_keep=[\"age\"]+[\"sex\"])\n",
    "from utils.fairness_utils import get_subroups_results, get_scores, get_subroups, get_subgroups_random\n",
    "dro_scores, sr = get_scores(gr[0], X_test0, y_test, S_test, y_pred>0.5, n_min=N_MIN_GROUP,\n",
    "              topK_DI=[1, 3], q_DI=10, topK_acc=[1], q_acc=4)\n",
    "print(dro_scores)\n",
    "sr.drop([\"group_label\",\"Acc by S\"], axis=1)[['n_obs','sex','age_q','Local DI','Local Acc']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f1dc2fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T23:29:46.736149Z",
     "start_time": "2024-04-26T23:29:46.685862Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m table \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val \u001b[38;5;241m=\u001b[39m \u001b[43mDATA_TRAIN_TEST2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     predicted, params, predictions_list \u001b[38;5;241m=\u001b[39m run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n\u001b[1;32m     15\u001b[0m     actual \u001b[38;5;241m=\u001b[39m test[dataset\u001b[38;5;241m.\u001b[39mget_class_attribute()]\u001b[38;5;241m.\u001b[39mvalues\n",
      "Cell \u001b[0;32mIn[152], line 2\u001b[0m, in \u001b[0;36mDATA_TRAIN_TEST2\u001b[0;34m(num, sens, y, columns_delete, scaling)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mDATA_TRAIN_TEST2\u001b[39m(num,sens,y,columns_delete,scaling):\n\u001b[0;32m----> 2\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDATASETS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# Adult data set\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     all_sensitive_attributes \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_sensitive_attributes_with_joint()\n\u001b[1;32m      4\u001b[0m     ProcessedData(dataset)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "algorithm = KamishimaAlgorithm() \n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'eta': 20} #params = algorithm.get_default_params()\n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(5,'X2','Y',['X2','Y'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "781630a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "supported tag: numerical-binsensitive\n",
      "sensitive attribute: race\n",
      "===================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'as_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     16\u001b[0m     train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val \u001b[38;5;241m=\u001b[39m DATA_TRAIN_TEST2(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m,[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 17\u001b[0m     predicted, params, predictions_list \u001b[38;5;241m=\u001b[39m \u001b[43mrun_alg\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_sensitive_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivileged_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     actual \u001b[38;5;241m=\u001b[39m test[dataset\u001b[38;5;241m.\u001b[39mget_class_attribute()]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     19\u001b[0m     y_pred2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(predicted)\n",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m, in \u001b[0;36mrun_alg\u001b[0;34m(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive, privileged_vals, positive_val)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#params = {'c': 10} \u001b[39;00m\n\u001b[1;32m      9\u001b[0m params \u001b[38;5;241m=\u001b[39m algorithm\u001b[38;5;241m.\u001b[39mget_default_params()\n\u001b[1;32m     10\u001b[0m predictions, predictions_list \u001b[38;5;241m=\u001b[39m  \\\n\u001b[0;32m---> 11\u001b[0m     \u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_sensitive_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                  \u001b[49m\u001b[43msingle_sensitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivileged_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions, params, predictions_list\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/zafar/ZafarAlgorithm.py:40\u001b[0m, in \u001b[0;36mZafarAlgorithmBase.run\u001b[0;34m(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals, params)\u001b[0m\n\u001b[1;32m     37\u001b[0m     out_file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m name\n\u001b[0;32m---> 40\u001b[0m train_name \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m test_name \u001b[38;5;241m=\u001b[39m create_file(test_df)\n\u001b[1;32m     42\u001b[0m fd, predictions_name \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mmkstemp()\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/zafar/ZafarAlgorithm.py:29\u001b[0m, in \u001b[0;36mZafarAlgorithmBase.run.<locals>.create_file\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_file\u001b[39m(df):\n\u001b[1;32m     28\u001b[0m     out \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 29\u001b[0m     out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mclass_attr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_matrix\u001b[49m()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     30\u001b[0m     out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m df[class_attr] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mas_matrix()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     31\u001b[0m     out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msensitive\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/site-packages/pandas/core/generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5983\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5984\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5985\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5986\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5987\u001b[0m ):\n\u001b[1;32m   5988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'as_matrix'"
     ]
    }
   ],
   "source": [
    "\n",
    "#####################   COMPAS  #####################\n",
    "\n",
    "\n",
    "algorithm = ZafarAlgorithmFairness()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    #params = {'c': 10} \n",
    "    params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac33d4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3770647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b1850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d12de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cdaa5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "10d88ac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T23:21:49.970570Z",
     "start_time": "2024-04-26T23:21:49.305540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "supported tag: numerical-binsensitive\n",
      "sensitive attribute: sex\n",
      "===================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'as_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     19\u001b[0m     train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val \u001b[38;5;241m=\u001b[39m DATA_TRAIN_TEST2(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincome-per-year\u001b[39m\u001b[38;5;124m\"\u001b[39m,[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincome-per-year\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m     predicted, params, predictions_list \u001b[38;5;241m=\u001b[39m \u001b[43mrun_alg\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_sensitive_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivileged_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     actual \u001b[38;5;241m=\u001b[39m test[dataset\u001b[38;5;241m.\u001b[39mget_class_attribute()]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     22\u001b[0m     y_pred2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(predicted)\n",
      "Cell \u001b[0;32mIn[158], line 14\u001b[0m, in \u001b[0;36mrun_alg\u001b[0;34m(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive, privileged_vals, positive_val)\u001b[0m\n\u001b[1;32m     11\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#params = algorithm.get_default_params()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m predictions, predictions_list \u001b[38;5;241m=\u001b[39m  \\\n\u001b[0;32m---> 14\u001b[0m     \u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_sensitive_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                  \u001b[49m\u001b[43msingle_sensitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivileged_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions, params, predictions_list\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/zafar/ZafarAlgorithm.py:40\u001b[0m, in \u001b[0;36mZafarAlgorithmBase.run\u001b[0;34m(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals, params)\u001b[0m\n\u001b[1;32m     37\u001b[0m     out_file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m name\n\u001b[0;32m---> 40\u001b[0m train_name \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m test_name \u001b[38;5;241m=\u001b[39m create_file(test_df)\n\u001b[1;32m     42\u001b[0m fd, predictions_name \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mmkstemp()\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/zafar/ZafarAlgorithm.py:29\u001b[0m, in \u001b[0;36mZafarAlgorithmBase.run.<locals>.create_file\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_file\u001b[39m(df):\n\u001b[1;32m     28\u001b[0m     out \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 29\u001b[0m     out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mclass_attr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_matrix\u001b[49m()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     30\u001b[0m     out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m df[class_attr] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mas_matrix()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     31\u001b[0m     out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msensitive\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/site-packages/pandas/core/generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5983\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5984\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5985\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5986\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5987\u001b[0m ):\n\u001b[1;32m   5988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'as_matrix'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##########################################################\n",
    "#################### Zafar 2017 B ##################\n",
    "##########################################################\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "ZafarAlgorithmBaseline\n",
    "algorithm = ZafarAlgorithmFairness()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'c': 0}\n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['sex','income-per-year'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "    \n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1720fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b0bc4558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T23:22:20.670331Z",
     "start_time": "2024-04-26T23:22:17.262347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "supported tag: numerical-binsensitive\n",
      "sensitive attribute: race\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/train_pr.py\", line 319, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/train_pr.py\", line 166, in main\n",
      "    clr = train(X, y, ns, opt)\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/train_pr.py\", line 128, in train\n",
      "    clr.fit(X, y, ns, itype=opt.itype)\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/fadm/lr/pr.py\", line 279, in fit\n",
      "    self.c_s_ = np.array([np.sum(s == si).astype(np.float)\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/fadm/lr/pr.py\", line 279, in <listcomp>\n",
      "    self.c_s_ = np.array([np.sum(s == si).astype(np.float)\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/numpy/__init__.py\", line 305, in __getattr__\n",
      "    raise AttributeError(__former_attrs__[attr])\n",
      "AttributeError: module 'numpy' has no attribute 'float'.\n",
      "`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/predict_lr.py\", line 278, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/vince/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/kamfadm-2012ecmlpkdd/predict_lr.py\", line 110, in main\n",
      "    clr = pickle.load(opt.model)\n",
      "EOFError: Ran out of input\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[159], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     15\u001b[0m     train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val \u001b[38;5;241m=\u001b[39m DATA_TRAIN_TEST2(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m,[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m     predicted, params, predictions_list \u001b[38;5;241m=\u001b[39m \u001b[43mrun_alg\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_sensitive_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivileged_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     actual \u001b[38;5;241m=\u001b[39m test[dataset\u001b[38;5;241m.\u001b[39mget_class_attribute()]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     18\u001b[0m     y_pred2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(predicted)\n",
      "Cell \u001b[0;32mIn[159], line 10\u001b[0m, in \u001b[0;36mrun_alg\u001b[0;34m(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive, privileged_vals, positive_val)\u001b[0m\n\u001b[1;32m      7\u001b[0m class_attr \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_class_attribute()\n\u001b[1;32m      8\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m200\u001b[39m} \u001b[38;5;66;03m#params = algorithm.get_default_params()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m predictions, predictions_list \u001b[38;5;241m=\u001b[39m  \\\n\u001b[0;32m---> 10\u001b[0m     \u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_sensitive_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                  \u001b[49m\u001b[43msingle_sensitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivileged_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions, params, predictions_list\n",
      "File \u001b[0;32m~/anaconda3/envs/Transf/lib/python3.8/site-packages/fairness/algorithms/kamishima/KamishimaAlgorithm.py:102\u001b[0m, in \u001b[0;36mKamishimaAlgorithm.run\u001b[0;34m(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals, params)\u001b[0m\n\u001b[1;32m     99\u001b[0m m \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mloadtxt(output_name)\n\u001b[1;32m    100\u001b[0m os\u001b[38;5;241m.\u001b[39munlink(output_name)\n\u001b[0;32m--> 102\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    103\u001b[0m predictions_correct \u001b[38;5;241m=\u001b[39m [class_type(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m predictions]\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions_correct, []\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "\n",
    "#####################   COMPAS  #####################\n",
    "\n",
    "\n",
    "algorithm = KamishimaAlgorithm()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'eta': 200} #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ee194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959e228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7abded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "import os\n",
    "import statistics\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from fairness import results\n",
    "from fairness.data.objects.list import DATASETS, get_dataset_names\n",
    "from fairness.data.objects.ProcessedData import ProcessedData\n",
    "from fairness.benchmark import run_alg\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "dataset = DATASETS[1] # Adult data set\n",
    "\n",
    "from fairness.algorithms.zafar.ZafarAlgorithm import ZafarAlgorithmBaseline, ZafarAlgorithmAccuracy, ZafarAlgorithmFairness\n",
    "from fairness.algorithms.kamishima.KamishimaAlgorithm import KamishimaAlgorithm\n",
    "from fairness.algorithms.kamishima.CaldersAlgorithm import CaldersAlgorithm\n",
    "from fairness.algorithms.feldman.FeldmanAlgorithm import FeldmanAlgorithm\n",
    "from fairness.algorithms.baseline.SVM import SVM\n",
    "from fairness.algorithms.baseline.DecisionTree import DecisionTree\n",
    "from fairness.algorithms.baseline.GaussianNB import GaussianNB\n",
    "from fairness.algorithms.baseline.LogisticRegression import LogisticRegression\n",
    "from fairness.algorithms.ParamGridSearch import ParamGridSearch\n",
    "from fairness.algorithms.Ben.SDBSVM import SDBSVM\n",
    "\n",
    "def p_rule(y_pred, z_values, threshold=0.5):\n",
    "    \n",
    "    y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n",
    "    y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\n",
    "    odds = y_z_1.mean() / y_z_0.mean()\n",
    "    return np.min([odds, 1/odds]) * 100\n",
    "\n",
    "\n",
    "class Sigmoid():\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return self.__call__(x) * (1 - self.__call__(x))\n",
    "    \n",
    "def DispFNR(y_pred, y, z_values, threshold=0.5):\n",
    "    ypred_z_1 = y_pred > threshold if threshold else y_pred[z_values == 1]\n",
    "    ypred_z_0 = y_pred > threshold if threshold else y_pred[z_values == 0]\n",
    "    result=abs(ypred_z_1[(y==1) & (z_values==0)].mean()-ypred_z_1[(y==1) & (z_values==1)].mean())\n",
    "    return result\n",
    "def DispFPR(y_pred, y, z_values, threshold=0.5):\n",
    "    ypred_z_1 = y_pred > threshold if threshold else y_pred[z_values == 1]\n",
    "    ypred_z_0 = y_pred > threshold if threshold else y_pred[z_values == 0]\n",
    "    result=abs(ypred_z_1[(y==0) & (z_values==0)].mean()-ypred_z_1[(y==0) & (z_values==1)].mean())\n",
    "    return result\n",
    "\n",
    "\n",
    "def DI(y_pred, z_values, threshold=0.5):\n",
    "    y_z_1 = y_pred[z_values == 1] > threshold if threshold else y_pred[z_values == 1]\n",
    "    y_z_0 = y_pred[z_values == 0] > threshold if threshold else y_pred[z_values == 0]\n",
    "    odds = abs(y_z_1.mean() - y_z_0.mean())\n",
    "    return odds\n",
    "\n",
    "def display_results(y_pred, y, sensitive):\n",
    "    y_pred2 = (y_pred>0.5).astype(int)\n",
    "    accuracy = accuracy_score(y, np.squeeze(y_pred2))\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"PRULE : \", p_rule(y_pred,sensitive))\n",
    "    print(\"DI : \", DI(y_pred, sensitive))\n",
    "    print(\"DispFPR : \", DispFPR(y_pred2, y, sensitive))\n",
    "    print(\"DispFNR : \", DispFNR(y_pred2, y, sensitive))\n",
    "    return {'Accuracy': accuracy, 'PRULE': p_rule(y_pred,sensitive), 'DispFPR': DispFPR(y_pred2, y, sensitive)\n",
    "            ,'DispFNR': DispFNR(y_pred2, y, sensitive)}\n",
    "\n",
    "def DATA_TRAIN_TEST(num,sens,y,columns_delete):\n",
    "    dataset = DATASETS[num] # Adult data set\n",
    "    all_sensitive_attributes = dataset.get_sensitive_attributes_with_joint()\n",
    "    ProcessedData(dataset)\n",
    "    processed_dataset = ProcessedData(dataset)\n",
    "    train_test_splits = processed_dataset.create_train_test_splits(1)\n",
    "    train_test_splits.keys()\n",
    "    train, test = train_test_splits['numerical-binsensitive'][0]\n",
    "    X_train = train\n",
    "    X_test = test\n",
    "    sensitive =  train[sens].values\n",
    "    sensitivet =  test[sens].values\n",
    "    y_train = train[y]\n",
    "    y_test = test[y]\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    s=X_train[sens]\n",
    "    st=X_test[sens]\n",
    "    t=X_train[y]\n",
    "    tt=X_test[y]\n",
    "    scale_df = lambda df, scaler: pd.DataFrame(scaler.transform(df), columns=df.columns, index=df.index)\n",
    "    X_train = X_train.pipe(scale_df, scaler)\n",
    "    X_test = X_test.pipe(scale_df, scaler)\n",
    "    X_train= X_train.drop([sens,y], axis=1)\n",
    "    X_train[sens] = s\n",
    "    X_train[y] = t\n",
    "    X_test= X_test.drop([sens,y], axis=1)\n",
    "    X_test[sens] = st\n",
    "    X_test[y] = tt\n",
    "\n",
    "    X_train = X_train.drop(columns_delete,1)\n",
    "    X_test = X_test.drop(columns_delete,1)\n",
    "    return X_train, X_test, y_train, y_test, sensitive, sensitivet\n",
    "\n",
    "\n",
    "def DATA_TRAIN_TEST2(num,sens,y,columns_delete,scaling):\n",
    "    dataset = DATASETS[num] # Adult data set\n",
    "    all_sensitive_attributes = dataset.get_sensitive_attributes_with_joint()\n",
    "    ProcessedData(dataset)\n",
    "    processed_dataset = ProcessedData(dataset)\n",
    "    train_test_splits = processed_dataset.create_train_test_splits(1)\n",
    "    train_test_splits.keys()\n",
    "    train, test = train_test_splits['numerical-binsensitive'][0]\n",
    "    #all_sensitive_attributes = dataset.get_sensitive_attributes_with_joint()\n",
    "    #for supported_tag in algorithm.get_supported_data_types():\n",
    "    supported_tag = 'numerical-binsensitive'\n",
    "    privileged_vals = dataset.get_privileged_class_names_with_joint(supported_tag)\n",
    "    positive_val = dataset.get_positive_class_val(supported_tag)\n",
    "    sensitive = sens\n",
    "    print(\"===================================\")\n",
    "    print(\"supported tag: \" + supported_tag)\n",
    "    print(\"sensitive attribute: \" + sensitive)\n",
    "    print(\"===================================\")\n",
    "    if scaling :\n",
    "        scaler = StandardScaler().fit(train)\n",
    "        s=train[sens]\n",
    "        st=test[sens]\n",
    "        t=train[y]\n",
    "        tt=test[y]\n",
    "        scale_df = lambda df, scaler: pd.DataFrame(scaler.transform(df), columns=df.columns, index=df.index)\n",
    "        train = train.pipe(scale_df, scaler)\n",
    "        test = test.pipe(scale_df, scaler)\n",
    "        train= train.drop(columns_delete, axis=1)\n",
    "        train[sens] = s\n",
    "        train[y] = t\n",
    "        test= test.drop(columns_delete, axis=1)\n",
    "        test[sens] = st\n",
    "        test[y] = tt\n",
    "\n",
    "    return train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def BANK(path):\n",
    "\n",
    "    column_names = ['age','job', 'marital', 'education', 'default', 'balance','housing', 'loan', 'contact','day','month','duration', 'campaign','pdays','previous', 'poutcome','y']\n",
    "    input_data = (pd.read_csv(path, \n",
    "                              na_values=\"?\", sep=r'\\s*;\\s*', engine='python'))\n",
    "\n",
    "\n",
    "    Z = (input_data['\"age\"'].astype(int) >= 50).astype(int)\n",
    "    y = (input_data['\"y\"'] == '\"yes\"').astype(int)\n",
    "    X = (input_data\n",
    "         .drop(columns=['\"y\"', '\"age\"','\"poutcome\"'])\n",
    "         .fillna('Unknown')\n",
    "         .pipe(pd.get_dummies, drop_first=True))\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = train_test_split(X, y, Z, test_size=0.2, \n",
    "                                                                     stratify=y, random_state=7)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, sensitive, sensitivet\n",
    "\n",
    "\n",
    "def DEFAULTS(path):\n",
    "\n",
    "    #column_names = ['age','job', 'marital', 'education', 'default', 'balance','housing', 'loan', 'contact','day','month','duration', 'campaign','pdays','previous', 'poutcome','y']\n",
    "    input_data = (pd.read_csv(path, \n",
    "                              na_values=\"?\", sep=r'\\s*;\\s*', engine='python'))\n",
    "\n",
    "    Z = input_data['X2']-1\n",
    "    y = input_data['Y']\n",
    "    X = (input_data\n",
    "         .drop(columns=['Y', 'X2'])\n",
    "         .fillna('Unknown')\n",
    "         .pipe(pd.get_dummies, drop_first=True))\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = train_test_split(X, y, Z, test_size=0.2, \n",
    "                                                                     stratify=y)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, sensitive, sensitivet\n",
    "\n",
    "\n",
    "##########################################################\n",
    "####################        Feldman     ##################\n",
    "####################        Feldman     ##################\n",
    "##########################################################\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "DecisionTree\n",
    "GaussianNB\n",
    "LogisticRegression\n",
    "algorithm = FeldmanAlgorithm(DecisionTree())  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'lambda': 1}\n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['sex','income-per-year'],False)\n",
    "    train['sex']=train['sex'].apply(str)\n",
    "    train['race-sex']=0\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   COMPAS  #####################\n",
    "\n",
    "\n",
    "algorithm = FeldmanAlgorithm(DecisionTree())  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'lambda': 0.01} \n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   DEFAULT  #####################\n",
    "\n",
    "\n",
    "algorithm = FeldmanAlgorithm(DecisionTree())  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'lambda': 0.01}    #DP\n",
    "    #params = {'lambda': 0.0001}   #EO\n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(5,'X2','Y',['Y'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "#####################   BANK  #####################\n",
    "\n",
    "\n",
    "algorithm = FeldmanAlgorithm(DecisionTree())  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'lambda': 1} \n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val  = DATA_TRAIN_TEST2(6,'age','Y',['age','Y'],True)    \n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "#################### Zafar 2017 B ##################\n",
    "##########################################################\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "ZafarAlgorithmBaseline\n",
    "algorithm = ZafarAlgorithmFairness()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'c': 0}\n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['sex','income-per-year'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "    \n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   COMPAS  #####################\n",
    "\n",
    "\n",
    "algorithm = ZafarAlgorithmFairness()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    #params = {'c': 10} \n",
    "    params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "#####################   DEFAULT  #####################\n",
    "\n",
    "algorithm = ZafarAlgorithmFairness() \n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    #params = {'c': 100} #`\n",
    "    params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(5,'X2','Y',['Y'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   BANK  #####################\n",
    "\n",
    "algorithm = ZafarAlgorithmFairness() \n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    #params = {'c': 100} #`\n",
    "    params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val  = DATA_TRAIN_TEST2(6,'age','Y',['age','Y'],True)    \n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "##########################################################\n",
    "#################### Zafar 2017 B ##################\n",
    "##########################################################\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "\n",
    "algorithm = ZafarAlgorithmAccuracy()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'gamma': 0.5}\n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['sex','income-per-year'],False)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   COMPAS  #####################\n",
    "\n",
    "\n",
    "algorithm = ZafarAlgorithmAccuracy() \n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'gamma': 0.015} #`\n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['two_year_recid'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "#####################   GERMAN  #####################\n",
    "\n",
    "\n",
    "algorithm = ZafarAlgorithmAccuracy() \n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'gamma': 0.0} #`\n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(2,'age','credit',['credit'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "#####################   DEFAULT  #####################\n",
    "\n",
    "\n",
    "algorithm = ZafarAlgorithmAccuracy() \n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'gamma': 0.001} #`\n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(5,'X2','Y',['Y'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "#####################   BANK  #####################\n",
    "\n",
    "\n",
    "algorithm = ZafarAlgorithmAccuracy() \n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'gamma': 0.5} #`\n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val  = DATA_TRAIN_TEST2(6,'age','Y',['age','Y'],True)    \n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "##########################################################\n",
    "#################### KamishimaAlgorithm ##################\n",
    "##########################################################\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "\n",
    "algorithm = KamishimaAlgorithm()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'eta': 19} # DP\n",
    "    #params = {'eta': 3} # EODDS\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['sex','income-per-year'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   COMPAS  #####################\n",
    "\n",
    "\n",
    "algorithm = KamishimaAlgorithm()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'eta': 200} #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "#####################   DEFAULT  #####################\n",
    "\n",
    "\n",
    "algorithm = KamishimaAlgorithm() \n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'eta': 20} #params = algorithm.get_default_params()\n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(5,'X2','Y',['X2','Y'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   BANK  #####################\n",
    "\n",
    "\n",
    "algorithm = KamishimaAlgorithm() \n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'eta': 10} #params = algorithm.get_default_params()\n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val  = DATA_TRAIN_TEST2(6,'age','Y',['age','Y'],True)    \n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "##########################################################\n",
    "#################### Zhang  Algorithm EQUALIZED ODDS ##################\n",
    "##########################################################\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "import tensorflow as tf\n",
    "import fairness\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =0.3,classifier_num_hidden_units=2,num_epochs=50)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "    \n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['sex','income-per-year'],True) \n",
    "    train = train.drop(['race-sex'], axis=1)\n",
    "    test = test.drop(['race-sex'], axis=1)\n",
    "\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   COMPAS  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'race': 1}]\n",
    "        unprivileged_groups = [{'race': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =7,classifier_num_hidden_units=40,num_epochs=75)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "    train = train.drop(['sex-race'], axis=1)\n",
    "    test = test.drop(['sex-race'], axis=1)\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "#####################   DEFAULT  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'X2': 1}]\n",
    "        unprivileged_groups = [{'X2': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =0.7,classifier_num_hidden_units=40,num_epochs=75)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(5,'X2','Y',['Y'],True)\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "#####################   BANK  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'age': 1}]\n",
    "        unprivileged_groups = [{'age': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =1,classifier_num_hidden_units=40,num_epochs=75)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['age']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val  = DATA_TRAIN_TEST2(6,'age','Y',['Y'],True)    \n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "#################### Wadsworth  Algorithm EQUALIZED ODDS ##################\n",
    "##########################################################\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.Wadsworth2018 import AdversarialDebiasing\n",
    "import tensorflow as tf\n",
    "import fairness\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =0.3,classifier_num_hidden_units=2,num_epochs=50)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "    \n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['sex','income-per-year'],True) \n",
    "    train = train.drop(['race-sex'], axis=1)\n",
    "    test = test.drop(['race-sex'], axis=1)\n",
    "\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   COMPAS  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.Wadsworth2018 import AdversarialDebiasing\n",
    "\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'race': 1}]\n",
    "        unprivileged_groups = [{'race': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =15,classifier_num_hidden_units=40,num_epochs=75)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "    train = train.drop(['sex-race'], axis=1)\n",
    "    test = test.drop(['sex-race'], axis=1)\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "#####################   DEFAULT  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.Wadsworth2018 import AdversarialDebiasing\n",
    "\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'X2': 1}]\n",
    "        unprivileged_groups = [{'X2': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =0.7,classifier_num_hidden_units=40,num_epochs=75)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(5,'X2','Y',['Y'],True)\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "#####################   BANK  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.Wadsworth2018 import AdversarialDebiasing\n",
    "\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'age': 1}]\n",
    "        unprivileged_groups = [{'age': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =0.3,classifier_num_hidden_units=40,num_epochs=75)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['age']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val  = DATA_TRAIN_TEST2(6,'age','Y',['Y'],True)    \n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "#################### Calders  Algorithm ##################\n",
    "##########################################################\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "\n",
    "algorithm = CaldersAlgorithm()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'beta': 0.01}#params = algorithm.get_default_params()\n",
    "    #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['income-per-year'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   COMPAS  #####################\n",
    "\n",
    "\n",
    "algorithm = CaldersAlgorithm()  # Kamishima algorithm\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = {'beta': 250} #params = algorithm.get_default_params()\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)        \n",
    "    return predictions, params, predictions_list\n",
    "table = [0,0,0,0]\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "    predicted, params, predictions_list = run_alg(algorithm, train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "#table=pd.crosstab(train[\"income-per-year\"],train[\"sex\"])\n",
    "#print(table)\n",
    "\n",
    "##########################################################\n",
    "####################    BI-ASED GBM     ##################\n",
    "##########################################################\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "table = [0,0,0,0]\n",
    "for k  in range(10):\n",
    "    np.random.seed()\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['income-per-year','race','race-sex'])\n",
    "    #X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(3,'race',\"two_year_recid\",['two_year_recid','sex','sex-race'])\n",
    "    \n",
    "    gb = GradientBoostingClassifier(n_estimators=500, learning_rate = 0.05, max_depth = 3, max_features=90,random_state=0,  min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "      min_samples_leaf=2, min_samples_split=2,\n",
    "      min_weight_fraction_leaf=0.0,\n",
    "      presort='auto')\n",
    "    gb.fit(X_train, y_train)\n",
    "    ##### Results on Train dataset #####\n",
    "    y_pred2= gb.predict(X_train)\n",
    "    print('Results on training set :')\n",
    "    display_results(y_pred2, y_train.values, sensitive)\n",
    "    ##### Results on Test dataset #####\n",
    "    y_predt2= gb.predict_proba(X_test.values)[:,1]\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "y_pred['y_pred_S1']=gb.predict_proba(X_train)[:,1][sensitive==1]\n",
    "y_pred['y_pred_S0']=gb.predict_proba(X_train)[:,1][sensitive==0]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_facecolor('white')\n",
    "ax1.grid(color='gray', linestyle=':', linewidth=1)\n",
    "plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "plt.rcParams[\"font.size\"] = 16.0\n",
    "ax1.hist(y_pred['y_pred_S1'], bins=100, density=1, histtype=\"step\", label=\"$p(f(X)|S=1)$\")\n",
    "ax1.hist(y_pred['y_pred_S0'], bins=100, density=1, histtype=\"step\", label=\"$p(f(X)|S=0)$\")\n",
    "leg = plt.legend(loc=\"best\")\n",
    "#plt.ylim(0,15)\n",
    "plt.xlabel(\"$f(X)$\")\n",
    "plt.ylabel(\"$p(f(X))$\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig(\"f-plain.pdf\")\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.8, 0.9),shadow=False, ncol=1)\n",
    "plt.show()\n",
    "plt.title('Unfair model (FAGTB with   =0.000)')\n",
    "plt.savefig(\"/Users/vincentgrari/Desktop/pdf/test.pdf\" , bbox_inches='tight')\n",
    "plt.clf() \n",
    "\n",
    "\n",
    "#####################   COMPAS  #####################\n",
    "table = [0,0,0,0]\n",
    "for k  in range(10):\n",
    "    #X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['income-per-year','sex','race-sex'])\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(3,'race',\"two_year_recid\",['two_year_recid','race','sex-race'])\n",
    "    \n",
    "    gb = GradientBoostingClassifier(n_estimators=500, learning_rate = 0.05, max_depth = 3, max_features=400,random_state=0,  min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "      min_samples_leaf=2, min_samples_split=2,\n",
    "      min_weight_fraction_leaf=0.0,\n",
    "      presort='auto')\n",
    "    gb.fit(X_train, y_train)\n",
    "    ##### Results on Train dataset #####\n",
    "    y_pred2= gb.predict(X_train)\n",
    "    print('Results on training set :')\n",
    "    display_results(y_pred2, y_train.values, sensitive)\n",
    "    ##### Results on Test dataset #####\n",
    "    y_predt2= gb.predict_proba(X_test.values)[:,1]\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "#####################   GERMAN  #####################\n",
    "table = [0,0,0,0]\n",
    "for k  in range(10):\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(2,'age',\"credit\",['age','credit','sex-age'])\n",
    "    y_train-=1\n",
    "    y_test-=1\n",
    "    gb = GradientBoostingClassifier(n_estimators=1000, learning_rate = 0.05, max_depth = 2, max_features=15,random_state=0,  min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "      min_samples_leaf=2, min_samples_split=2,\n",
    "      min_weight_fraction_leaf=0.0,\n",
    "      presort='auto')\n",
    "    gb.fit(X_train, y_train)\n",
    "    ##### Results on Train dataset #####\n",
    "    y_pred2= gb.predict(X_train)\n",
    "    print('Results on training set :')\n",
    "    display_results(y_pred2, y_train.values, sensitive)\n",
    "    ##### Results on Test dataset #####\n",
    "    y_predt2= gb.predict_proba(X_test.values)[:,1]\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "#####################   BANK  #####################\n",
    "table = [0,0,0,0]\n",
    "for k  in range(10):\n",
    "    np.random.seed()\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(6,'age',\"Y\",[ 'Y'])    \n",
    "    gb = GradientBoostingClassifier(n_estimators=800, learning_rate = 0.05, max_depth = 3, max_features=35,random_state=0,  min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "      min_samples_leaf=2, min_samples_split=2,\n",
    "      min_weight_fraction_leaf=0.0,\n",
    "      presort='auto')\n",
    "    gb.fit(X_train, y_train)\n",
    "    ##### Results on Train dataset #####\n",
    "    y_pred2= gb.predict(X_train)\n",
    "    print('Results on training set :')\n",
    "    display_results(y_pred2, y_train.values, np.array(sensitive.astype(int)))\n",
    "    ##### Results on Test dataset #####\n",
    "    y_predt2= gb.predict_proba(X_test.values)[:,1]\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "#####################   DEFAULTS  #####################\n",
    "table = [0,0,0,0]\n",
    "for k  in range(10):\n",
    "    np.random.seed()\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(5,'X2',\"Y\",['Y','X2'])\n",
    "    gb = GradientBoostingClassifier(n_estimators=800, learning_rate = 0.05, max_depth = 4, max_features=20,random_state=0,  min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "      min_samples_leaf=2, min_samples_split=2,\n",
    "      min_weight_fraction_leaf=0.0,\n",
    "      presort='auto')\n",
    "    gb.fit(X_train, y_train)\n",
    "    ##### Results on Train dataset #####\n",
    "    y_pred2= gb.predict(X_train)\n",
    "    print('Results on training set :')\n",
    "    display_results(y_pred2, y_train.values, np.array(sensitive.astype(int)))\n",
    "    ##### Results on Test dataset #####\n",
    "    y_predt2= gb.predict_proba(X_test.values)[:,1]\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "####################    BI-ASED NN      ##################\n",
    "##########################################################\n",
    "\n",
    "    \n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "learning_rate = 0.0001\n",
    "batch_no = len(X_train) // batch_size\n",
    "model = NN()\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['income-per-year','race','race-sex','sex'])\n",
    "X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(3,'race',\"two_year_recid\",['two_year_recid','sex','sex-race','race'])\n",
    "\n",
    "#96\n",
    "class NN(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(399, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = self.fc3(x)\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    if epoch % 5 == 0:\n",
    "        x_train = Variable(torch.FloatTensor(X_train.values),requires_grad=True)\n",
    "        x_test = Variable(torch.FloatTensor(X_test.values),requires_grad=True)\n",
    "        print('Epoch {}'.format(epoch+1))\n",
    "        y_pred2= (model(x_train).data.numpy().T>0.5).astype(int)\n",
    "        accuracy = accuracy_score(y_train, np.squeeze(y_pred2))\n",
    "        print (\"Train Accuracy:\", accuracy)\n",
    "    x_train, ytrain = shuffle(X_train.values,np.expand_dims(y_train,axis = 1))\n",
    "    # Mini batch learning\n",
    "    for i in range(batch_no):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        x_var = Variable(torch.FloatTensor(x_train[start:end]))\n",
    "        y_var = Variable(torch.FloatTensor(ytrain[start:end]))\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        ypred_var = model(x_var)\n",
    "        loss =criterion(ypred_var, y_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "##### Results on Train dataset #####\n",
    "x_train = Variable(torch.FloatTensor(X_train.values),requires_grad=True)\n",
    "y_pred2= (torch.sigmoid(model(x_train)).data.numpy())\n",
    "print('Results on training set :')\n",
    "display_results(y_pred2, y_train.values, sensitive)\n",
    "##### Results on Test dataset #####\n",
    "x_test = Variable(torch.FloatTensor(X_test.values),requires_grad=True)\n",
    "y_predt2= (torch.sigmoid(model(x_test)).data.numpy())\n",
    "print('')\n",
    "print('Results on test set :')\n",
    "display_results(y_predt2, y_test.values, sensitivet)\n",
    "\n",
    "\n",
    "\n",
    "#####################   GERMAN  #####################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "learning_rate = 0.0001\n",
    "batch_no = len(X_train) // batch_size\n",
    "\n",
    "\n",
    "#96\n",
    "class NN(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(57, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 1)        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = self.fc3(x)\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "model = NN()\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#X_train, X_test, y_train, y_test, sensitive, sensitivet= BANK('/Users/vincentgrari/anaconda3/lib/python3.6/site-packages/fairness/data/preprocessed/bankfull_categorical-binsensitive.csv')\n",
    "X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(2,'age',\"credit\",['age','credit','sex-age'])\n",
    "y_train-=1\n",
    "y_test-=1\n",
    "\n",
    "table = [0,0,0,0]\n",
    "for k  in range(10):\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch % 5 == 0:\n",
    "            x_train = Variable(torch.FloatTensor(X_train.values),requires_grad=True)\n",
    "            x_test = Variable(torch.FloatTensor(X_test.values),requires_grad=True)\n",
    "            print('Epoch {}'.format(epoch+1))\n",
    "            y_pred2= (model(x_train).data.numpy().T>0.5).astype(int)\n",
    "            accuracy = accuracy_score(y_train, np.squeeze(y_pred2))\n",
    "            print (\"Train Accuracy:\", accuracy)\n",
    "        x_train, ytrain = shuffle(X_train.values,np.expand_dims(y_train,axis = 1))\n",
    "        # Mini batch learning\n",
    "        for i in range(batch_no):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            x_var = Variable(torch.FloatTensor(x_train[start:end]))\n",
    "            y_var = Variable(torch.FloatTensor(ytrain[start:end]))\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            ypred_var = model(x_var)\n",
    "            loss =criterion(ypred_var, y_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    ##### Results on Train dataset #####\n",
    "    x_train = Variable(torch.FloatTensor(X_train.values),requires_grad=True)\n",
    "    y_pred2= (torch.sigmoid(model(x_train)).data.numpy())\n",
    "    print('Results on training set :')\n",
    "    display_results(y_pred2, y_train.values, sensitive)\n",
    "    ##### Results on Test dataset #####\n",
    "    x_test = Variable(torch.FloatTensor(X_test.values),requires_grad=True)\n",
    "    y_predt2= (torch.sigmoid(model(x_test)).data.numpy())\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "    \n",
    "  \n",
    "\n",
    "#####################   DEFAULT  #####################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "batch_size = 32\n",
    "num_epochs = 500\n",
    "learning_rate = 0.0001\n",
    "batch_no = len(X_train) // batch_size\n",
    "\n",
    "\n",
    "#96\n",
    "class NN(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(22, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "model = NN()\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#X_train, X_test, y_train, y_test, sensitive, sensitivet= BANK('/Users/vincentgrari/anaconda3/lib/python3.6/site-packages/fairness/data/preprocessed/bankfull_categorical-binsensitive.csv')\n",
    "X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(5,'X2',\"Y\",['Y','X2'])\n",
    "\n",
    "\n",
    "table = [0,0,0,0]\n",
    "for k  in range(10):\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch % 5 == 0:\n",
    "            x_train = Variable(torch.FloatTensor(X_train.values),requires_grad=True)\n",
    "            x_test = Variable(torch.FloatTensor(X_test.values),requires_grad=True)\n",
    "            print('Epoch {}'.format(epoch+1))\n",
    "            y_pred2= (model(x_train).data.numpy().T>0.5).astype(int)\n",
    "            accuracy = accuracy_score(y_train, np.squeeze(y_pred2))\n",
    "            print (\"Train Accuracy:\", accuracy)\n",
    "        x_train, ytrain = shuffle(X_train.values,np.expand_dims(y_train,axis = 1))\n",
    "        # Mini batch learning\n",
    "        for i in range(batch_no):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            x_var = Variable(torch.FloatTensor(x_train[start:end]))\n",
    "            y_var = Variable(torch.FloatTensor(ytrain[start:end]))\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            ypred_var = model(x_var)\n",
    "            loss =criterion(ypred_var, y_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    ##### Results on Train dataset #####\n",
    "    x_train = Variable(torch.FloatTensor(X_train.values),requires_grad=True)\n",
    "    y_pred2= (torch.sigmoid(model(x_train)).data.numpy())\n",
    "    print('Results on training set :')\n",
    "    display_results(y_pred2, y_train.values, sensitive)\n",
    "    ##### Results on Test dataset #####\n",
    "    x_test = Variable(torch.FloatTensor(X_test.values),requires_grad=True)\n",
    "    y_predt2= (torch.sigmoid(model(x_test)).data.numpy())\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "  \n",
    "\n",
    "#####################   BANK  #####################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "learning_rate = 0.0001\n",
    "batch_no = len(X_train) // batch_size\n",
    "\n",
    "\n",
    "#96\n",
    "class NN(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(38, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 1)        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "model = NN()\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#X_train, X_test, y_train, y_test, sensitive, sensitivet= BANK('/Users/vincentgrari/anaconda3/lib/python3.6/site-packages/fairness/data/preprocessed/bankfull_categorical-binsensitive.csv')\n",
    "X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(6,'age',\"Y\",['age', 'Y'])    \n",
    "\n",
    "\n",
    "table = [0,0,0,0]\n",
    "for k  in range(10):\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch % 5 == 0:\n",
    "            x_train = Variable(torch.FloatTensor(X_train.values),requires_grad=True)\n",
    "            x_test = Variable(torch.FloatTensor(X_test.values),requires_grad=True)\n",
    "            print('Epoch {}'.format(epoch+1))\n",
    "            y_pred2= (model(x_train).data.numpy().T>0.5).astype(int)\n",
    "            accuracy = accuracy_score(y_train, np.squeeze(y_pred2))\n",
    "            print (\"Train Accuracy:\", accuracy)\n",
    "        x_train, ytrain = shuffle(X_train.values,np.expand_dims(y_train,axis = 1))\n",
    "        # Mini batch learning\n",
    "        for i in range(batch_no):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            x_var = Variable(torch.FloatTensor(x_train[start:end]))\n",
    "            y_var = Variable(torch.FloatTensor(ytrain[start:end]))\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            ypred_var = model(x_var)\n",
    "            loss =criterion(ypred_var, y_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    ##### Results on Train dataset #####\n",
    "    x_train = Variable(torch.FloatTensor(X_train.values),requires_grad=True)\n",
    "    y_pred2= (torch.sigmoid(model(x_train)).data.numpy())\n",
    "    print('Results on training set :')\n",
    "    display_results(y_pred2, y_train.values, sensitive)\n",
    "    ##### Results on Test dataset #####\n",
    "    x_test = Variable(torch.FloatTensor(X_test.values),requires_grad=True)\n",
    "    y_predt2= (torch.sigmoid(model(x_test)).data.numpy())\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "##########################################################\n",
    "###############    PYTORCH FAIR GTB LOGISTIC      ########\n",
    "###############     DEMOGRAPHIC PARITY            ########\n",
    "##########################################################\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "\n",
    "import copy\n",
    "# 315 trees for PC FIXE\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['sex','income-per-year','race-sex']) #,'race','race-sex','sex'\n",
    "    #X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(3,'race',\"two_year_recid\",['two_year_recid','sex','sex-race','race'])\n",
    "    \n",
    "    classifier= GradientBoosting_AXA_Fair(n_estimators=300, learning_rate = 0.01, max_depth = 10,min_samples_split=1.0, min_impurity =False, max_features =20, regression =1)\n",
    "    y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=0.15, Xtest=X_test.values, yt=y_test,\n",
    "                      sensitivet=sensitivet,adv=Logistic())\n",
    "    \n",
    "    ##### Results on Test dataset #####\n",
    "    y_predt2 = classifier.predict(X_test.values)\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    #display_results(y_predt2, y_test.values, sensitivet)\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "#####################   COMPAS  #####################\n",
    "\n",
    "\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    np.random.seed()\n",
    "    #X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['sex','income-per-year','race-sex']) #,'race','race-sex','sex'\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(3,'race',\"two_year_recid\",['two_year_recid','sex-race','race'])\n",
    "    \n",
    "    classifier= GradientBoosting_AXA_Fair(n_estimators=700, learning_rate = 0.01, max_depth = 12,min_samples_split=1.0, min_impurity =False, max_features =400, regression =1)\n",
    "    y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=0.27, Xtest=X_test.values, yt=y_test,\n",
    "                      sensitivet=sensitivet,adv=Logistic())\n",
    "    \n",
    "    ##### Results on Test dataset #####\n",
    "    y_predt2 = classifier.predict(X_test.values)\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    #display_results(y_predt2, y_test.values, sensitivet)\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "#####################   DEFAULT  #####################\n",
    "\n",
    "\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    np.random.seed()\n",
    "    #X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['sex','income-per-year','race-sex']) #,'race','race-sex','sex'\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(5,'X2',\"Y\",['Y','X2'])\n",
    "    classifier= GradientBoosting_AXA_Fair(n_estimators=350, learning_rate = 0.01, max_depth = 9,min_samples_split=1.0, min_impurity =False, max_features =22, regression =1)\n",
    "    y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=0.2, Xtest=X_test.values, yt=y_test,\n",
    "                      sensitivet=sensitivet,adv=Logistic())\n",
    "    \n",
    "    ##### Results on Test dataset #####\n",
    "    y_predt2 = classifier.predict(X_test.values)\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    #display_results(y_predt2, y_test.values, sensitivet)\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "#####################   BANK  #####################\n",
    "\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    np.random.seed()\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(6,'age',\"Y\",['age', 'Y'])    \n",
    "    classifier= GradientBoosting_AXA_Fair(n_estimators=800, learning_rate = 0.01, max_depth = 4,min_samples_split=1.0, min_impurity =False, max_features =22, regression =1)\n",
    "    y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=0.2, Xtest=X_test.values, yt=y_test,\n",
    "                      sensitivet=sensitivet,adv=Logistic())\n",
    "    \n",
    "    ##### Results on Test dataset #####\n",
    "    y_predt2 = classifier.predict(X_test.values)\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    #display_results(y_predt2, y_test.values, sensitivet)\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "###############    PYTORCH FAIR GTB LOGISTIC      ########\n",
    "###############         EQUALITY OF ODDS          ########\n",
    "##########################################################\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "\n",
    "# 315 trees for PC FIXE\n",
    "table = [0,0,0,0]\n",
    "for i in range(1):\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['sex','income-per-year','race-sex']) #,'race','race-sex','sex'\n",
    "    #X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(3,'race',\"two_year_recid\",['two_year_recid','sex','sex-race','race'])\n",
    "    \n",
    "    classifier= GradientBoosting_AXA_Fair(n_estimators=415, learning_rate = 0.01, max_depth = 10,min_samples_split=1.0, min_impurity =False, max_features =20, regression =1)\n",
    "    y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=0.1, Xtest=X_test.values, yt=y_test,\n",
    "                      sensitivet=sensitivet,adv=Logistic())\n",
    "    \n",
    "    ##### Results on Test dataset #####\n",
    "    y_predt2 = classifier.predict(X_test.values)\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    #display_results(y_predt2, y_test.values, sensitivet)\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "X_input: np.concatenate([y_pred2,y], axis=1)\n",
    "A = np.concatenate([ np.expand_dims(sensitive,axis=1),np.expand_dims(y_train,axis=1)], axis=1).T\n",
    "A[0,].shape\n",
    "#####################   COMPAS  #####################\n",
    "\n",
    "\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    #X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['sex','income-per-year','race-sex']) #,'race','race-sex','sex'\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(3,'race',\"two_year_recid\",['two_year_recid','sex-race','race'])\n",
    "    \n",
    "    classifier= GradientBoosting_AXA_Fair(n_estimators=2000, learning_rate = 0.01, max_depth = 3,min_samples_split=1.0, min_impurity =False, max_features =400, regression =1)\n",
    "    y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=0.15, Xtest=X_test.values, yt=y_test,\n",
    "                      sensitivet=sensitivet,adv=Logistic())\n",
    "    \n",
    "    ##### Results on Test dataset #####\n",
    "    y_predt2 = classifier.predict(X_test.values)\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    #display_results(y_predt2, y_test.values, sensitivet)\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   DEFAULT  #####################\n",
    "\n",
    "\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    #X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['sex','income-per-year','race-sex']) #,'race','race-sex','sex'\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(5,'X2',\"Y\",['Y','X2'])\n",
    "    \n",
    "    classifier= GradientBoosting_AXA_Fair(n_estimators=700, learning_rate = 0.01, max_depth = 6,min_samples_split=1.0, min_impurity =False, max_features =22, regression =1)\n",
    "    y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=0.01, Xtest=X_test.values, yt=y_test,\n",
    "                      sensitivet=sensitivet,adv=Logistic())\n",
    "    \n",
    "    ##### Results on Test dataset #####\n",
    "    y_predt2 = classifier.predict(X_test.values)\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    #display_results(y_predt2, y_test.values, sensitivet)\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   BANK  #####################\n",
    "\n",
    "\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    #X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['sex','income-per-year','race-sex']) #,'race','race-sex','sex'\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(6,'age',\"Y\",['age', 'Y'])    \n",
    "    \n",
    "    classifier= GradientBoosting_AXA_Fair(n_estimators=700, learning_rate = 0.01, max_depth = 5,min_samples_split=1.0, min_impurity =False, max_features =38, regression =1)\n",
    "    y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=0.1, Xtest=X_test.values, yt=y_test,\n",
    "                      sensitivet=sensitivet,adv=Logistic())\n",
    "    \n",
    "    ##### Results on Test dataset #####\n",
    "    y_predt2 = classifier.predict(X_test.values)\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    #display_results(y_predt2, y_test.values, sensitivet)\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "##########################################################\n",
    "###############        PYTORCH FAIR GTB           ########\n",
    "#############          NEURAL NETWORK            #########\n",
    "##########################################################\n",
    "\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['sex','income-per-year','race-sex']) #,'race','race-sex','sex'\n",
    "    #X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(3,'race',\"two_year_recid\",['two_year_recid','sex','sex-race','race'])\n",
    "    \n",
    "    classifier= GradientBoosting_AXA_Fair(n_estimators=1120, learning_rate = 0.01, max_depth = 10,min_samples_split=1.0, min_impurity =False, max_features =20, regression =1)\n",
    "    y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=300, Xtest=X_test.values, yt=y_test,\n",
    "                      sensitivet=sensitivet,adv=Logistic())\n",
    "    \n",
    "    ##### Results on Test dataset #####\n",
    "    y_predt2 = classifier.predict(X_test.values)\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    #display_results(y_predt2, y_test.values, sensitivet)\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   COMPAS  #####################\n",
    "\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    #X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['sex','income-per-year','race-sex']) #,'race','race-sex','sex'\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(3,'race',\"two_year_recid\",['two_year_recid','sex-race','race'])\n",
    "    \n",
    "    classifier= GradientBoosting_AXA_Fair(n_estimators=1500, learning_rate = 0.01, max_depth = 12,min_samples_split=1.0, min_impurity =False, max_features =400, regression =1)\n",
    "    y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=300, Xtest=X_test.values, yt=y_test,\n",
    "                      sensitivet=sensitivet,adv=Logistic())\n",
    "    \n",
    "    ##### Results on Test dataset #####\n",
    "    y_predt2 = classifier.predict(X_test.values)\n",
    "    print('')\n",
    "    print('Results on test set :')\n",
    "    #display_results(y_predt2, y_test.values, sensitivet)\n",
    "    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "###########################################################\n",
    "###############        RELASCED FAIR GTB           ########\n",
    "#############         LOGISTIC RAGRESSION            ######\n",
    "###########################################################\n",
    "\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "\n",
    "table = [0,0,0,0]\n",
    "for i in range(1):\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['sex','income-per-year','race-sex']) #,'race','race-sex','sex'\n",
    "    #X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(3,'race',\"two_year_recid\",['two_year_recid','sex','sex-race','race'])\n",
    "    \n",
    "    classifier= GradientBoosting_AXA_Fair(n_estimators=3000, learning_rate = 0.01, max_depth = 10,min_samples_split=1.0, min_impurity =False, max_features =20, regression =1)\n",
    "    y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=600, Xtest=X_test.values, yt=y_test,\n",
    "                      sensitivet=sensitivet)\n",
    "    \n",
    "    ##### Results on Test dataset #####\n",
    "#    y_predt2 = classifier.predict(X_test.values,sensitivet)\n",
    "#    print('')\n",
    "#    print('Results on test set :')\n",
    "    #display_results(y_predt2, y_test.values, sensitivet)\n",
    "#    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "#    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "#np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   COMPAS  #####################\n",
    "\n",
    "table = [0,0,0,0]\n",
    "for i in range(5):\n",
    "    #X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(1,'sex',\"income-per-year\",['sex','income-per-year','race-sex']) #,'race','race-sex','sex'\n",
    "    X_train, X_test, y_train, y_test, sensitive, sensitivet = DATA_TRAIN_TEST(3,'race',\"two_year_recid\",['two_year_recid','sex-race','race'])\n",
    "    \n",
    "    classifier= GradientBoosting_AXA_Fair(n_estimators=1480, learning_rate = 0.01, max_depth = 12,min_samples_split=1.0, min_impurity =False, max_features =400, regression =1)\n",
    "    y_pred = classifier.fit(X_train.values, y_train.values, sensitive, LAMBDA=150, Xtest=X_test.values, yt=y_test,\n",
    "                      sensitivet=sensitivet)\n",
    "    \n",
    "    ##### Results on Test dataset #####\n",
    "#    y_predt2 = classifier.predict(X_test.values)\n",
    "#    print('')\n",
    "#    print('Results on test set :')\n",
    "#    #display_results(y_predt2, y_test.values, sensitivet)\n",
    "#    Res = display_results(y_predt2, y_test.values, sensitivet)\n",
    "#    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "#np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "##########################################################\n",
    "####################    HARDT  Algorithm   ###############\n",
    "       \n",
    "#####################   ADULT  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "\n",
    "# first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "import fairness\n",
    "import fairness.benchmark\n",
    "\n",
    "class EqOddsPostprocessing2(Algorithm):\n",
    "    def __init__(self, ypred):\n",
    "        self.name = 'Hardt-EqOddsPostprocessing2'\n",
    "        self.prediction = ypred\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "        EOPP = EqOddsPostprocessing(privileged_groups=privileged_groups,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    seed=42)\n",
    "        data_orig_test_pred = test_df.copy(deepcopy=True)\n",
    "        data_orig_test_pred.labels = self.prediction\n",
    "\n",
    "        EOPP = EOPP.fit(test_df, data_orig_test_pred)\n",
    "        data_transf_test_pred = EOPP.predict(test_df)\n",
    "        return data_transf_test_pred.labels, []\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['sex','income-per-year'],True) \n",
    "    train = train.drop(['race-sex'], axis=1)\n",
    "    test = test.drop(['race-sex'], axis=1)\n",
    "\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate = 0.05, max_depth = 3, max_features=90,random_state=0,  min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "      min_samples_leaf=2, min_samples_split=2,\n",
    "      min_weight_fraction_leaf=0.0,\n",
    "      presort='auto')\n",
    "    train2 = train.copy(True)\n",
    "    del train2['sex']\n",
    "    del train2['income-per-year']\n",
    "    gb.fit(train2, train['income-per-year'])\n",
    "    ##### Results on Train dataset #####\n",
    "    test2 = test.copy(True)\n",
    "    del test2['sex']\n",
    "    del test2['income-per-year']\n",
    "    y_pred2= gb.predict_proba(test2)[:,1]\n",
    "\n",
    "    print(\"===============================\")\n",
    "    print(\"Before Hardt post-processing\")\n",
    "    print(\"===============================\")\n",
    "    display_results(y_pred2, test[dataset.get_class_attribute()].values, test[sensitive].values)\n",
    "\n",
    "    fairness.add_algorithm(EqOddsPostprocessing2(y_pred2))\n",
    "    algorithm = EqOddsPostprocessing2(y_pred2) \n",
    "    test2 = test.copy(True)\n",
    "    #del test2['sex']\n",
    "    #del test2['income-per-year']\n",
    "    #test_ibm.labels =  np.expand_dims(gb.predict(test2),axis=1)\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "\n",
    "    z_values = test[sensitive].values # comment this line after it was executed once and run the block again\n",
    "\n",
    "    print(\"===============================\")\n",
    "    print(\"After Hardt post-processing\")\n",
    "    print(\"===============================\")\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "\n",
    "# first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "import fairness\n",
    "import fairness.benchmark\n",
    "\n",
    "class EqOddsPostprocessing2(Algorithm):\n",
    "    def __init__(self, ypred):\n",
    "        self.name = 'Hardt-EqOddsPostprocessing2'\n",
    "        self.prediction = ypred\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "        EOPP = EqOddsPostprocessing(privileged_groups=privileged_groups,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    seed=42)\n",
    "        data_orig_test_pred = train_df.copy(deepcopy=True)\n",
    "        data_orig_test_pred.scores = self.prediction\n",
    "\n",
    "        EOPP = EOPP.fit(train_df, data_orig_test_pred)\n",
    "        data_transf_test_pred = EOPP.predict(train_df)\n",
    "        return data_transf_test_pred, []\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['sex','income-per-year'],True) \n",
    "    train = train.drop(['race-sex'], axis=1)\n",
    "    test = test.drop(['race-sex'], axis=1)\n",
    "\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate = 0.05, max_depth = 3, max_features=90,random_state=0,  min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "      min_samples_leaf=2, min_samples_split=2,\n",
    "      min_weight_fraction_leaf=0.0,\n",
    "      presort='auto')\n",
    "    train2 = train.copy(True)\n",
    "    del train2['sex']\n",
    "    del train2['income-per-year']\n",
    "    gb.fit(train2, train['income-per-year'])\n",
    "    ##### Results on Train dataset #####\n",
    "    y_pred2= gb.predict_proba(train2)[:,1]\n",
    "\n",
    "    print(\"===============================\")\n",
    "    print(\"Before Hardt post-processing\")\n",
    "    print(\"===============================\")\n",
    "    display_results(y_pred2, train[dataset.get_class_attribute()].values, train[sensitive].values)\n",
    "\n",
    "    fairness.add_algorithm(EqOddsPostprocessing2(y_pred2))\n",
    "    algorithm = EqOddsPostprocessing2(y_pred2) \n",
    "    test2 = test.copy(True)\n",
    "    del test2['sex']\n",
    "    del test2['income-per-year']\n",
    "    #test_ibm.labels =  np.expand_dims(gb.predict(test2),axis=1)\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted.scores)\n",
    "\n",
    "    z_values = test[sensitive].values # comment this line after it was executed once and run the block again\n",
    "\n",
    "    print(\"===============================\")\n",
    "    print(\"After Hardt post-processing\")\n",
    "    print(\"===============================\")\n",
    "    #display_results(y_pred2, train[dataset.get_class_attribute()].values, train[sensitive].values)\n",
    "    display_results(y_pred2, train[dataset.get_class_attribute()].values, train[sensitive].values)\n",
    "\n",
    "    #Res = display_results(y_pred2, actual, z_values)\n",
    "    #table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "##########################################################\n",
    "#################### Wadsworth Demographic Parity ##################\n",
    "##########################################################\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "import tensorflow as tf\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.WadsworthDP import AdversarialDebiasing\n",
    "import fairness\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =150.)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "    \n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['sex','income-per-year'],True) \n",
    "    train = train.drop(['race-sex'], axis=1)\n",
    "    test = test.drop(['race-sex'], axis=1)\n",
    "\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   COMPAS  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.WadsworthDP import AdversarialDebiasing\n",
    "\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'race': 1}]\n",
    "        unprivileged_groups = [{'race': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =180, num_epochs=100)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "    train = train.drop(['sex-race'], axis=1)\n",
    "    test = test.drop(['sex-race'], axis=1)\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "#####################   DEFAULT  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.WadsworthDP import AdversarialDebiasing\n",
    "\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'X2': 1}]\n",
    "        unprivileged_groups = [{'X2': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =1.)#,adversary_loss_weight =180, num_epochs=100)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['X2']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(5,'X2','Y',['Y'],True)\n",
    "\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "#####################   BANK  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.WadsworthDP import AdversarialDebiasing\n",
    "\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'age': 1}]\n",
    "        unprivileged_groups = [{'age': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =2, num_epochs=50)#,adversary_loss_weight =180, num_epochs=100)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['age']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val  = DATA_TRAIN_TEST2(6,'age','Y',['Y'],True)    \n",
    "\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "##########################################################\n",
    "#################### Zhang Demographic Parity ##################\n",
    "##########################################################\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "import tensorflow as tf\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasingDP import AdversarialDebiasing\n",
    "import fairness\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =1.)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "    \n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['sex','income-per-year'],True) \n",
    "    train = train.drop(['race-sex'], axis=1)\n",
    "    test = test.drop(['race-sex'], axis=1)\n",
    "\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   COMPAS  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasingDP import AdversarialDebiasing\n",
    "\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'race': 1}]\n",
    "        unprivileged_groups = [{'race': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =120, num_epochs=100)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "    train = train.drop(['sex-race'], axis=1)\n",
    "    test = test.drop(['sex-race'], axis=1)\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "#####################   Default  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "import tensorflow as tf\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasingDP import AdversarialDebiasing\n",
    "import fairness\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'X2': 1}]\n",
    "        unprivileged_groups = [{'X2': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =1.)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "    \n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['X2']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(5,'X2','Y',['Y'],True)\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################   BANK  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "import tensorflow as tf\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasingDP import AdversarialDebiasing\n",
    "import fairness\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'age': 1}]\n",
    "        unprivileged_groups = [{'age': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =2, num_epochs=50)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "    \n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['age']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val  = DATA_TRAIN_TEST2(6,'age','Y',['Y'],True)    \n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "##########################################################\n",
    "#################### Zhang Equalized of odds #############\n",
    "##########################################################\n",
    "\n",
    "#####################   ADULT  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "import tensorflow as tf\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "import fairness\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =100.)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "    \n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['sex','income-per-year'],True) \n",
    "    train = train.drop(['race-sex'], axis=1)\n",
    "    test = test.drop(['race-sex'], axis=1)\n",
    "\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "\n",
    "y_pred2[(z_values==1) & (actual==0)].mean()\n",
    "y_pred2[(z_values==0) & (actual==0)].mean()\n",
    "\n",
    "\n",
    "y_pred2[(z_values==1) & (actual==1)].mean()-y_pred2[(z_values==0) & (actual==1)].mean()\n",
    "(y_pred2[(z_values==1) & (actual==1)]==0).mean()-(y_pred2[(z_values==0) & (actual==1)]==0).mean()\n",
    "\n",
    "\n",
    "TPR\n",
    "FNR\n",
    "\n",
    "DispFNR(y_pred2, actual, z_values)\n",
    "DispFPR(y_pred2, actual, z_values)\n",
    "\n",
    "\n",
    "#####################   COMPAS  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasingDP import AdversarialDebiasing\n",
    "\n",
    "class AdversarialDebiasing2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'Zhang-AdversarialDebiasing2'\n",
    "\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        privileged_groups = [{'race': 1}]\n",
    "        unprivileged_groups = [{'race': 0}]\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups, scope_name='debiased_classifier',\n",
    "                                              debias=True, sess=sess,adversary_loss_weight =120, num_epochs=100)\n",
    "\n",
    "        debiased_model.fit(train_df)\n",
    "        dataset_debiasing_test = debiased_model.predict(test_df)\n",
    "        sess.close()\n",
    "        return dataset_debiasing_test.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "fairness.add_algorithm(AdversarialDebiasing2())\n",
    "algorithm = AdversarialDebiasing2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(3,'race','two_year_recid',['race','two_year_recid'],True)\n",
    "    train = train.drop(['sex-race'], axis=1)\n",
    "    test = test.drop(['sex-race'], axis=1)\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "##########################################################\n",
    "####################        Feldman     ##################\n",
    "##########################################################\n",
    "#####################   ADULT  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing.disparate_impact_remover import DisparateImpactRemover\n",
    "import fairness\n",
    "class DisparateImpactRemover2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'disparate_impact_remover'\n",
    "        self.level =level\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        level =1.\n",
    "        protected = 'sex'\n",
    "        index = train_df.feature_names.index(protected)\n",
    "        di = DisparateImpactRemover(repair_level=level)\n",
    "        train_repd = di.fit_transform(train_df)\n",
    "        test_repd = di.fit_transform(test_df)\n",
    "\n",
    "\n",
    "        X_tr = np.delete(train_repd.features, index, axis=1)\n",
    "        X_te = np.delete(test_repd.features, index, axis=1)\n",
    "        y_tr = train_repd.labels.ravel()\n",
    "        \n",
    "        lmod = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
    "        lmod.fit(X_tr, y_tr)\n",
    "        \n",
    "        test_repd_pred = test_repd.copy()\n",
    "        test_repd_pred.labels = lmod.predict(X_te)\n",
    "\n",
    "        return test_repd_pred.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "fairness.add_algorithm(DisparateImpactRemover2())\n",
    "algorithm = DisparateImpactRemover2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(10):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['sex','income-per-year'],True) \n",
    "    #dataset.sensitive_attrs=['sex']\n",
    "    train = train.drop(['race-sex'], axis=1)\n",
    "    test = test.drop(['race-sex'], axis=1)\n",
    "\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=dataset.sensitive_attrs)\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "#####################   COMPAS  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing.disparate_impact_remover import DisparateImpactRemover\n",
    "import fairness\n",
    "class DisparateImpactRemover2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'disparate_impact_remover'\n",
    "        self.level =level\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        level =1.\n",
    "        protected = 'sex'\n",
    "        index = train_df.feature_names.index(protected)\n",
    "        di = DisparateImpactRemover(repair_level=level)\n",
    "        train_repd = di.fit_transform(train_df)\n",
    "        test_repd = di.fit_transform(test_df)\n",
    "\n",
    "\n",
    "        X_tr = np.delete(train_repd.features, index, axis=1)\n",
    "        X_te = np.delete(test_repd.features, index, axis=1)\n",
    "        y_tr = train_repd.labels.ravel()\n",
    "        \n",
    "        lmod = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
    "        lmod.fit(X_tr, y_tr)\n",
    "        \n",
    "        test_repd_pred = test_repd.copy()\n",
    "        test_repd_pred.labels = lmod.predict(X_te)\n",
    "\n",
    "        return test_repd_pred.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "fairness.add_algorithm(DisparateImpactRemover2())\n",
    "algorithm = DisparateImpactRemover2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "fairness.add_algorithm(DisparateImpactRemover2())\n",
    "algorithm = DisparateImpactRemover2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['sex']\n",
    "table = [0, 0, 0, 0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val = DATA_TRAIN_TEST2(1,'sex',\"income-per-year\",['sex','income-per-year'],True) \n",
    "    train = train.drop(['race-sex'], axis=1)\n",
    "    test = test.drop(['race-sex'], axis=1)\n",
    "    train = train.drop(['race'], axis=1)\n",
    "    test = test.drop(['race'], axis=1)\n",
    "\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=['sex'])\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=['sex'])\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "for k in range(100):\n",
    "    print(np.array(predicted.features[k])-np.array(train_ibm.features[k]))\n",
    "\n",
    "\n",
    "#####################   BANK  #####################\n",
    "sys.path.append('/Users/vincentgrari/Desktop/AIF360/')\n",
    "sys.path.append('/Users/vincentgrari/Desktop/FAIR2/')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing.disparate_impact_remover import DisparateImpactRemover\n",
    "import fairness\n",
    "class DisparateImpactRemover2(Algorithm):\n",
    "    def __init__(self):\n",
    "        self.name = 'disparate_impact_remover'\n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs, single_sensitive, privileged_vals,\n",
    "            params):\n",
    "        level =1\n",
    "        protected = 'age'\n",
    "        index = train_df.feature_names.index(protected)\n",
    "        di = DisparateImpactRemover(repair_level=level)\n",
    "        train_repd = di.fit_transform(train_df)\n",
    "        test_repd = di.fit_transform(test_df)\n",
    "\n",
    "\n",
    "        X_tr = np.delete(train_repd.features, index, axis=1)\n",
    "        X_te = np.delete(test_repd.features, index, axis=1)\n",
    "        y_tr = train_repd.labels.ravel()\n",
    "        \n",
    "        lmod = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
    "        lmod.fit(X_tr, y_tr)\n",
    "        \n",
    "        test_repd_pred = test_repd.copy()\n",
    "        test_repd_pred.labels = lmod.predict(X_te)\n",
    "        \n",
    "        gb = GradientBoostingClassifier(n_estimators=500, learning_rate = 0.05, max_depth = 3, max_features=30,random_state=0,  min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "          min_samples_leaf=2, min_samples_split=2,\n",
    "          min_weight_fraction_leaf=0.0,\n",
    "          presort='auto')\n",
    "        gb.fit(X_tr, y_tr)\n",
    "        ##### Results on Train dataset #####\n",
    "        test_repd_pred.labels= gb.predict(X_te)\n",
    "\n",
    "        return test_repd_pred.labels, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        return set([\"numerical-binsensitive\"])\n",
    "\n",
    "fairness.add_algorithm(DisparateImpactRemover2())\n",
    "algorithm = DisparateImpactRemover2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['age']\n",
    "table = [0, 0, 0, 0]\n",
    "fairness.add_algorithm(DisparateImpactRemover2())\n",
    "algorithm = DisparateImpactRemover2() # first add new algorithm to fairness-comparison (run adversarial_for_fairness_comparison.py)\n",
    "all_sensitive_attributes = ['age']\n",
    "table = [0, 0, 0, 0]\n",
    "for i in range(5):\n",
    "    train, test, dataset, all_sensitive_attributes, sensitive, privileged_vals, positive_val  = DATA_TRAIN_TEST2(6,'age','Y',['Y'],False)\n",
    "\n",
    "    train_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=train, label_names=[dataset.class_attr], protected_attribute_names=['age'])\n",
    "    test_ibm = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=test, label_names=[dataset.class_attr], protected_attribute_names=['age'])\n",
    "\n",
    "    (predicted, params, predictions_list) = run_alg(algorithm, train_ibm, test_ibm, dataset, all_sensitive_attributes,\n",
    "                                                    sensitive, privileged_vals, positive_val)\n",
    "    actual = test[dataset.get_class_attribute()].values\n",
    "    y_pred2 = np.asarray(predicted)\n",
    "    z_values = test[sensitive].values\n",
    "    Res = display_results(y_pred2, actual, z_values)\n",
    "    table = np.vstack([table,[Res['Accuracy']*100,Res['PRULE'],Res['DispFPR'],Res['DispFNR']]])\n",
    "\n",
    "np.savetxt(sys.stdout, np.mean(table[1:,], axis=0).astype(float), '%5.2f')\n",
    "\n",
    "\n",
    "for k in range(100):\n",
    "    print(np.array(predicted.features[k])-np.array(train_ibm.features[k]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env-kernel",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
